TODO: translated from French with DeepL, so needs proofreading and markdown (or LaTeX) formatting...



# Lexicon

## Code for the level of various terms

- A: fundamental, best understood before, or during, an introductory data science course

- B: important, to be understood by the end of an introductory data science course

- C1: interesting, but peripheral to the subject (general culture)

- C2: interesting, directly related to the topic, but more complex than the topic (general culture)

- D: advanced, complex and interesting and related to the topic (to give vision and perspective; it's good to "have heard of it")



## Data science jobs

- Data wrangler (A): individual knowing how to create, synthesize, prepare, normalize and/or homogenize data so that it is coherent and can be processed by algorithms.

- Data engineer (A): individual knowing how to store, distribute and route data so that it can be processed by algorithms on specific computing machines. A software engineer specialized in the handling of data.

- Data analyst/scientist (A): individual who knows how to analyze input data, formalize a problem and a problematic, choose an algorithmic protocol for data processing, and finally evaluate the quality of the predictive model generated by the algorithm.

- ML researcher (A): individual who uses mathematical, data science and engineering principles to invent new machine learning algorithms, or improve existing ones (performance improvement, broadening of the application domain, etc).

- ML engineer (A): individual who uses data science and machine learning principles to implement existing ML algorithms to concrete business cases, e.g. automatic optimal logistics management.



## Computer science and programming

- Algorithm (A): mathematical process capable of transforming inputs (data in input) into outputs (data in output), according to a determined and structured protocol.

- Heuristic (B): a trick of algorithmic design that allows to trade precision on the solution for a faster calculation time for very complex problems.

- Programming language (A): a programming language is a way of writing text, usually a hybrid between English and mathematical language, in order to translate mathematical/algorithmic ideas into a form that an electronic computer can understand and execute.

- Machine language (A): a language based on the binary alphabet (containing just 2 symbols, '0' and '1'; or 'current is not flowing' and 'current is flowing'), allowing a computer to process information automatically.

- Type (A): the mathematical space to which a computer concept belongs. It is one of the most important concepts in computer science, both practical and theoretical. Mathematically, a type corresponds to the mathematical space to which a computer quantity belongs. We can distinguish for example the type "Integer" (relative integers), the type "Float" (floating point number, serving as an approximation of real numbers), the type "String" (textual character strings).

- Code (A): code is text written in a programming language.

- Syntax (A): the grammatical structure of a language. The syntax of a programming language can for example, depending on the language, declare the type before a variable ("Type_A variable_a") or after ("variable_a:Type_A). An instruction ("sentence" of a programming language) that does not respect the syntax of its language will not be executable by the machine, because it is untranslatable in machine language.

- Semantics (A): the meaning taken by an instruction/phrase in a language. An example of a semantic error would be `Integer my_variable = "bobo"`: "bobo" is not an integer, so the statement, while syntactically correct, is semantically incorrect. Language example: "Colorless green ideas are sleeping furiously" (Chomsky); again, not a grammatical error, but clearly an error of meaning.

- Variable (computer science sense) (A): in computer science, a name declared to be able to store a mathematical value in memory, and use it conceptually. (A "5.75" that hangs around is often confusing; a "Float speed = 5.75" followed by the adapted and repeated use of "speed" is much clearer to read).

- Function (computer sense) (A): A function is a series of instructions callable from other places in the code, which can take zero, one, or more inputs and return zero, one, or more outputs. Functions can also affect the state (memory) of a program or computer (side effects).

- Condition (A): A condition is a statement that translates a logical calculation (Boolean, a question with a true/false answer) into a conditional redirection of the code. Keywords: if, elif, else, and, or, not, then.

- Loop (A): A loop is a series of instructions that can be repeated, with minimal configurable changes, as long as a given condition remains true.

- Argument (A): an argument is a synonym for an atomic part of a function's input.

- Return (A): a return is a synonym for a function's output.

- Signature (A): the signature of a function is the declaration of the name of the function, and the types and names of its inputs (and often outputs as well).



- Array/List (A): an array, or list, is a series of values, usually of the same type, stored (usually) contiguously in memory, and accessible by index. In the vast majority of languages, the indexing of an array starts at 0. For example if `Array<Integer> my_array = [1, 4, 6, 10, 0]`, then `array[2]` returns 6.

- Tensor (B): a tensor is an array of arrays of arrays of arrays of arrays... of elements usually of the same type. The "rank" of a tensor is its nesting level. A simple value is a tensor of rank 0. A simple array is a tensor of rank 1. An array of arrays is a rank 2 tensor. An array of array of array is a rank 3 tensor. Etc.

- Graph (A): A graph is a set of points (usually of the same type), linked together by 2-to-2 links.

- Data point cloud / Data Frame / Data point space (A): A two-dimensional table, with "individuals" as rows, and "attributes" as columns, corresponds geometrically to a point cloud in a mathematical frame, where each column of the table defines a dimension, i.e. an axis of the frame. Each individual corresponds precisely to a point in this frame. Most data tables have too many dimensions for the human brain to visualize (since we are limited to 3 spatial dimensions, 1 temporal dimension, and possibly 1 or 2 color gradients, for a total of 6 very rarely attainable in an understandable way).

- (Relational) database (C2): a database is a set of (usually massive) data tables, possibly logically linked together by 2-column tables called "junction tables". Relational databases (the reason why most people eat Excel tables all day long) have an advanced mathematical formalism, called "relational algebra". This model was developed by Edgar Codd.

- Vectorization (C2): Vectorization is the expression of computations on arrays in such a way that the computations can take place simultaneously (this is called "parallelism"), according to the SIMD (Single Instruction, Multiple Data) design. It is this principle that allows GPUs to do graphical computation, or computation for data or neural networks faster than CPUs (processors).

- Interpreted language, compiled language (C1): a language is said to be "interpreted", if a software called "the interpreter", must be launched and read it line by line so that this one executes. A language is said to be "compiled", if a software called the "compiler", must read the whole code and produce an executable before it can be executed. Python is an example of an interpreted language. C is a compiled language.

- Static typing, dynamic typing (C1): A typing is said to be static if it is necessary for the syntax of a programming language. A typing is said to be dynamic if the compiler or interpreter is able to infer the type of a computer value from its context, and thus the declaration of the type of a variable or the arguments of a function is not necessary. Python is a dynamically typed language. C is a statically typed language. Statically typed languages are generally more restrictive, but also more rigorous and reliable (i.e., there is less debugging to do). https://pythonconquerstheuniverse.wordpress.com/2009/10/03/static-vs-dynamic-typing-of-programming-languages/

- Paradigm (of a programming language): way to design a programming language. The 3 main paradigms are: imperative, object-oriented, and functional. They are not necessarily mutually exclusive. Imperative is very close to the machine, ordering concrete electronic instructions (typical example: C). The object oriented structures its components in classes, containing both data (names) and functions (verbs) (most modern languages are inspired by the object oriented, Python included, among others). The functional is very close to mathematics and is inspired by lambda calculus; it has the advantage of producing very solid code, because it is close to a mathematical proof in its treatment by the interpreter or the compiler.

- Turing-completeness (C1): roughly speaking, a programming language is said to be "Turing-complete" if it is able to execute computations on all types, store information in its memory, execute its computations contextually/conditionally, and redirect its reading thread. Turing-completeness is a fundamental notion of theoretical computer science, and for reasons that are long and complex to explain, it is in a way the ultimate "speed limit" to computation. A Turing-complete language/architecture is able to execute any Turing-complete function/language. There are 5 families of Turing-complete architectures to date: the Von Neumann architecture, the Turing machine, lambda calculus, some cellular automata, and neural networks.


## Data science frameworks or languages

- Python (A): Python is a programming language designed (diazoed) to look a lot like plain English, to be simple to learn, to be dynamically typed, to be interpreted, and to have a richness of syntactic expression. This makes it a very good language for code discovery and experimentation/prototyping. However, the fact that this language is sometimes quite slow at runtime, not easy to develop cross-platform, and dynamically typed; makes it generally avoided in production. It is the language of research, learning, scripting and prototyping.

- Jupyter (A): Technology for launching a local server capable of running Python in a browser. Very useful for Python development, given the problems of cross-platform distribution of Python, as well as those in the production of Python executables.

- Numpy (B): Standard linear algebra library in Python. https://www.datacamp.com/community/blog/python-numpy-cheat-sheet

- Scipy (B): Standard scientific computing library in Python. https://www.datacamp.com/community/blog/python-scipy-cheat-sheet

- SciKit Learn (B): Standard Machine Learning library in Python. https://www.datacamp.com/community/tutorials/machine-learning-python

- Pandas (B): Library for managing dataframes (data tables) in Python. Very useful, quite standard in data science. https://www.datacamp.com/community/blog/python-pandas-cheat-sheet

- Matplotlib (B): Complete but not very pretty data visualization library in Python. A bit complex to get into at first, but essential to know. https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet

- Seaborn (B): Data visualization library specialized in data science. Does everything very "classical" very well, simply, and nicely. Difficult or impossible to do custom visualizations sometimes. https://www.datacamp.com/community/tutorials/seaborn-python-tutorial

- SQL (C1): Basic database query language. Replaced since by languages that descend from it often ajd (PostGresQL, GraphQL), even if many databases still work with SQL. https://campus.datacamp.com/courses/introduction-to-sql/selecting-columns?ex=1



## General mathematics

- Set (mathematical sense) (A): a set is a collection of mathematical objects. The binary alphabet is the set containing the symbols 0 and 1, noted {0, 1}. The natural integers form a set, noted N = {0, 1, 2, 3, ...}. 

- Space / algebraic structure (mathematical sense) (A): a space is a set with laws/properties and/or operators on its elements. The '+' operator on the binary language (set of all combinations of symbols of the binary alphabet noted {0, 1}* = {"", "0", "1", "00", "01", "10", "11", "000", "001", ...}) is concatenation, and glues the "words" together (e.g., "1010" + "111" = "1010111") The '+' operator in natural numbers is addition (e.g., 5 + 7 = 7 + 5 = 12). An example of a property is commutativity: a + b = b + a. This property is verified in the natural integers, but not in the binary language.

- Function (mathematical sense) (A): a function is an association of the elements of a set of inputs (called a domain) with a set of outputs (called a codomain), where each input has at most one output image, so that the result of a function, given a fixed input, is deterministic. The square root is an example of a 1 input, 1 output function. The classical operators are examples of 2 input, 1 output functions (for example, note +(5, 7) = 5 + 7 = 12, and this fact becomes clearer).

- Application: You can consider that "application" and "function" are synonymous terms in mathematics (by abuse of language, even if there is a technical distinction in French).

- Image: the image y by a function f of an element x of the domain is the unique element y = f(x) of the codomain.

- Antecedent: an antecedent x of an element y of the codomain of a function f is an element of the domain such that y = f(x).

- Variable (mathematical sense) (A): a variable is a value typed, but not specified, to represent the space in which it exists in its entirety.

- Parameter (A): a parameter is a variable which is given a fixed value, but which could just as easily be changed.

- Function composition: if f is a function from A to B and g is a function from B to C, then there exists a function `h = g ° f` from A to C, such that h is the concatenation of f and then g, where the return of the function f is given as an argument to g

- Continuous function (A): a function is said to be continuous if its graphical representation (linking its inputs and outputs) has no "break".

- Derivable function (A): a derivable function is a continuous function which does not have any "corners" in its graphical representation.

- Derivative (A): the derivative of a function is another function, representing the slope of rise or fall at each point of this function as a numerical value. A rise for an input point corresponds to a positive value of the derivative at that same input point; a fall to a negative value.

- Exponential function (A): the exponential function is the "fundamental" function whose derivative is the exponential function itself. Among its many properties, one `exp(a + b) = exp(a) * exp(b)`, i.e., it transforms an addition in input into a multiplication in output. https://fr.wikipedia.org/wiki/Fonction_exponentielle

- Logarithmic function / Neperian logarithm (A): the reciprocal of the exponential function. The derivative of the neperian logarithm is the function `(x -> 1/x)`. Also, `ln (a * b) = ln(a) + ln(b)`, the neperian logarithm turns a product into a sum. https://en.wikipedia.org/wiki/Logarithm

- Logistic function (B): The logistic function is a continuous, strictly increasing function, varying from 0 to 1 (on its outputs) from -inf to +inf (on its inputs). https://fr.wikipedia.org/wiki/Fonction_logistique_(Verhulst)

- Integral (C1): the integral is the "rule" (in the sense of a tool to measure) of mathematics. It is also the "anti-derivative".

- Symbolic notation of iterated sums and products (B): A large SIGMA represents an iterated sum, a large PI represents an iterated product. Example: `Σ_{i = 0}^{i = 5} 2^i = 1 + 2 + 4 + 8 + 16 + 32`


## Mathematics for statistics

- Vector space (B): a vector space E is an algebraic structure with commutative addition and subtraction ("abelian group"), accompanied by another structure K called "body", itself having usual addition, subtraction, multiplication and division (except by 0), allowing to change the size of vectors. This structure is coupled with a series of laws that allow the combined operation of K and E in a correct way. https://fr.wikipedia.org/wiki/Espace_vectoriel

- Vector addition (A): addition of E * E -> E (2 inputs in E, one output in E)

- Vector scaling (A): multiplication of K * E -> E (1 input in K, 1 input in E, 1 output in E)

- Scalar (A): a scalar is an element of K. In general, we choose K = R, the field of real numbers. A scalar is a tensor of rank 0.

- Vector (A): a vector is an element of E. In general, a vector will be an array of n elements. The number n is then shared by all elements of E and is called the "dimension" of E. E is then denoted R^n (for the vector space of real numbers of dimension n). The functions of R in R are also an example (more complex) of vectors, because they respect the same laws. The space (R -> R) is a vector space. A vector is a tensor of rank 1.

- Linear combination: any possible arrangement of scalings and additions of a set of vectors. For example, if we have 3 vectors u, v, and w, then 72 * u + 1/3 * v - 4.5 * w is a linear combination of vectors u, v, and w. We can always simplify a linear combination back to a form where each vector is multiplied by a single scalar, and the list of these scalar-vector products is summed.

- Linear independence (A): a set of n vectors is linearly independent if the only way to get the zero vector (the zero of E, the element that changes nothing by vector addition) by linear combination is to have the scalar factor in front of each scalar-vector pair be zero. Intuitively, this means that each vector in the set contributes to creating a new dimension, independent of those created by the other vectors.

- Basis: a set of linearly independent vectors that can describe any point in a vector space. There are always precisely as many elements in a basis as there are dimensions in a vector space.

- Dimension (A): number of elements in any basis of a vector space; number of axes needed to represent a vector space.

- Colinearity: two vectors are said to be colinear if their direction describes the same line passing through the origin.

- Linearity / Linear map (A): Said of a function from E into F, where E and F are vector spaces, that sends the origin of E (its zero vector) to the origin of F, and keeps all parallel lines in E parallel in F. Algebraically, a function/application is said to be linear if and only if f(0_E) = 0_F and f(ku+v) = kf(u) + f(v), for all k in K and all u and v in E. Geometrically something "linear" is something straight (a line, a plane, etc); something non-linear is something curved (a sphere, a parabola, etc). Linear often also refers to polynomials of degree 1 (which give straight forms), quadratic to polynomials of degree 2, cubic to polynomials of degree 3, etc.

- Matrix (A): a matrix is a rectangle of scalars. A matrix of size `m * n` (m rows, n columns) represents a linear map of R^n -> R^m (i.e. an input, a vector of dimension n, and an output, a vector of dimension m). A matrix is a tensor of rank 2.

- Matrix multiplication: matrix multiplication is the composition of linear maps.

- Dot product (A): the dot product is a product of E * E -> K (2 vectors of E as input, one scalar of K as output). It is computed as the sum of the term-to-term product of each coordinate of its two inputs. It is denoted `<u, v>`, and is also equal to `||u|| * ||v|| * cos(u,v)`. The scalar product algebraically encodes two pieces of geometric information (one about angles, and one about lengths). In practice, the scalar product of two vectors of norm 1 is equal to the angle between these two vectors; the scalar product of two collinear vectors is equal to the multiplication of their norm.

- Norm (A): The norm of a vector is the distance of displacement that this vector represents. It is usually defined from the scalar product, as `||u|| = sqrt(<u, u>)` (Euclidean norm, also called norm 2, the one from the Pythagorean theorem). There are different norms (useful in ML), like the "maximal norm/infinite norm" or the "Manhattan norm/1 norm".

- Quadratic norm (A): the quadratic norm is the norm of a vector squared. It is usually defined as `||u||^2 = <u, u>`

- Cosine (A): value used to define the angle between 2 vectors. Algebraically, `cos(u, v) = <u, v> / (|u| * |v|)`

- Distance (metric) (B): function to define a measure of the distance between two vectors. If the vector space is normed (has a norm function), a distance can always be defined from it as d(u, v) = ||u - v||.

- Span (generated vector subspace) (B): the vector subspace generated by a family of vectors is the set of points that are reachable by linear combinations of the vectors of this family.

- Normalization (linear algebra) (A): vector scaling of a vector u by the value 1/||u||, in order to find a vector û of norm 1 and the same direction as u.

- Tensor (B): A tensor is an arrangement of scalars; a tensor of rank zero is a point (a single scalar), a tensor of rank one is a row (a vector (column vector) or a covector (row vector)), a tensor of rank two is a square (a matrix), a tensor of rank three is a cube (hypermatrix), etc.

- Linear form / covector (C1): A linear form is a function of E -> K. Linear forms are line vectors.



- Vector subspace (B): A vector subspace is a vector space contained in another vector space. A vector line, or a vector plane (i.e. passing through the origin) in a vector space of dimension 3 are examples of a vector subspace. Any space is a vector subspace of itself.

- Hyperplane (B): is a vector subspace of dimension n-1 in a vector space of dimension n. The 2d planes are the hyperplanes of the 3d space. The 1d lines are the hyperplanes of the 2d planes. The role of a hyperplane is to separate a vector space into 2 pieces. For example, any "mirror" symmetry is done with respect to a hyperplane, whatever the dimension n of the enclosing space.

- Eigenvectors/Eigenvalues (C2): Any linear map has vector subspaces that are stable (i.e. if an input is in that vector subspace, then so is the output by the linear map in question). Linear maps in vector spaces amount to dilations. The vectors serving as the basis for these vector subspaces are called eigenvectors; their respective dilation coefficient is called an eigenvalue.

- Euclidean space (B): vector space with a scalar product, allowing it to define a norm (the Euclidean norm) and therefore a distance (the Euclidean distance).

- Normed vector space (B): vector space having a norm, allowing it to define a distance.

- Metric space (B): vector space having a distance.



- Universe of possibilities (A): set describing the set of possible outcomes of a random experiment. For example, the universe of possibilities for a normal roll of a die (1d6) is O = { 1, 2, 3, 4, 5, 6 }.

- Probability (A): A probability is a function on a universe of possibilities (possible outcomes listed as a set) that returns an outcome between 0 (0%) and 1 (100%). This universe of possibilities is geometrically an object of measure 1 (a length of 1 in dimension 1, an area of 1 in dimension 2, a volume of 1 in dimension 3, an n-hypervolume of 1 in dimension n).

- Conditional probability (Bayesian probabilities) (A): A conditional probability is a probability knowing that an event A is true. This is geometrically equivalent to restricting oneself to the section of the universe of possibilities where A is true, and consider that this new geometric object is of measure 1.

- Random variable (A): probabilistic experiment where the result is assigned a value (in general a real number). Algebraically, a random variable is a function of the universe of possibilities in (in general) the set of real numbers, usually denoted X : O -> R. For example, if I win 0.50€ per point on a 1d6 die, but if I roll a 6, I lose 3€, our function looks like X = { 1 -> 0.5 ; 2 -> 1 ; 3 -> 1.5 ; 4 -> 2 ; 5 -> 2.5 ; 6 -> -3 }.

- Random vector (B): probabilistic experiment where the outcome is assigned multiple values (usually n real numbers). For example, if I refer to the value of two normal throws of the dice in order (2d6) as "x" for the first throw, and "y" for the second, then the example of a random vector where I win 2x candies but lose 0.5xy€ on each pair of throws can be defined as X = (x, y) -> (2x, 0.5xy). Models in data science are usually random vectors that try to match the shape and properties of a cloud of data points, if the probabilistic experiment is performed repeatedly on different points.

- Expectation (A): average of the results of a universe, weighted by their probability. It is the "return that can be expected in the long run".

- Variance (A): Average of the squared deviations of each of the outcomes from the expectation. The variance can also be understood as the covariance of a random variable (or vector) with itself, i.e. the quadratic norm of a random variable. In the case of a random vector, the variance takes the form of a symmetric, positive semidefinite matrix, called the variance-covariance matrix. https://fr.wikipedia.org/wiki/Variance_(math%C3%A9matiques)

- Standard deviation (A): the standard deviation is the square root of the variance of a variable or a random vector. It is therefore the norm of a random vector. noted sigma_X

- Covariance (B): The covariance is a measure of how much two random variables vary together. It is the scalar product of spaces of random vectors. It is noted cov(X, Y)

- Pearson's correlation coefficient (B): measure of how closely two events are correlated. It is the cosine of the spaces of random vectors. It is defined as cor(X, Y) = cov(X, Y) / (stddev(X) * stddev(Y))

- Bias (A): difference between the mean/expectation given by the model, and the one given by the experiment (the real data).



- Scalar function (B): A scalar function is a function from R^n -> R that is not necessarily linear. Ex: f : R^3 -> R, such that (x, y, z) -> xy + z^2 + cos(x)

- Partial derivative: derivative of a scalar function considering all coordinates constant, except one. Ex, going with the previous function: df/dx = y - sin(x)

- Gradient (B): The gradient is the "fundamental" derivative of a scalar function. The gradient indicates the direction in which to move to get the largest increase in the output value. The gradient is calculated as the vector of all partial derivatives. Ex, going with the previous function: NABLA_f = (df/dx, df/dy, df/dz) = (y - sin(x), x, 2z). 

- Multivariate function: function from R^n -> R^m, not necessarily linear. Ex: f : R^3 -> R^2 such that (x,y,z) -> = (xy + cos(x), xyz + z^2). Note that we can write f(x,y,z) = (f_1(x,y,z), f_2(x,y,z)), where f_1(x,y,z) = xy + cos(x) and f_2(x,y,z) = xyz + z^2 are both scalar functions

- Jacobian: the "fundamental" derivative of a multivariate function. The Jacobian is a matrix of size m * n, where the i-th row corresponds to the gradient of the scalar function corresponding to the set of inputs, and the i-th output. To caricature, "we stack the gradients". Note that the Jacobian is itself a function of R^n -> (R^n -> R^m); that is, your Jacobian gives you a different matrix for each point in your space where you compute it. Geometrically, the Jacobian gives you the best linear approximation at a point (x,y,z) of its return in the output space.
Ex, going with the previous function:
```
J_f =
    [ df_1 / dx,  df_1 / dy, df_1 / dz ] = [ y - sin(y),     x,        0       ]
    [ df_2 / dx,  df_2 / dy, df_2 / dz ]   [ yz,             xz,       xy + 2z ]
```


- Manifold (C2): a manifold is an object on which we can define a geometry on which functions can exist. Any vector space, as well as any space of functions between vector spaces, is a manifold. The sphere, the torus, the Klein bottle, the n-dimensional Cartesian plane (R^n), are all examples of manifolds.

- Topology (C2): branch of mathematics that studies the local and global structures of manifolds. Also, a term for a "fabric of reality" based on the language of sets, on such spaces, in order to define notions of distance, measure, continuity, derivability, etc.

- Measure theory (C2): extension of topology to build a powerful version of the integral, and to define a way to compute lengths, areas, volumes, whatever the dimension. Probabilities are geometrically understood as "n-volumes relative to a global space of n-volume 1", and measure theory is thus the foundation of all modern probabilities.

- Topological data analysis (D): a technique that aims to study the geometric form (the manifold) on which data points exist in an encompassing statistical vector space, in order to drastically reduce its dimension or to make its logical structure explicit. 

- Differential Geometry (D): theory of derivation on manifolds.

- Information Geometry (D): mathematical theory relying on probability, statistics, information theory, measure theory and differential geometry to establish statistical spaces (random vector spaces; data point cloud spaces) and dynamics on manifolds.



## Machine learning

- ML fields of study: Natural Language Processing (NLP); Computer Vision; Decision-making; Business analytics; Ranking; Darwinian algorithms/Reinforcement learning, etc

- Supervised Learning (A): learning where the data is all labeled and specified by a human being, so that the algorithm can know if it is right or wrong.

- Unsupervised Learning (A): learning where the algorithm itself is supposed to classify, measure or label the data.

- Reinforcement Learning (A): learning where the algorithm itself perceives its environment, and learns according to it in a Darwinian way to learn to perform a task more and more efficiently.

- Regression: a type of model learning that seeks to predict numerical values for fictitious input data, close to the real output data in relation to its input data.

- Classification: a type of model learning that seeks to predict an output categorization of an input data point. 

- Model (A): a model is a mathematical function used as a hypothesis to generate a virtual data point from a chosen input. The goal of a model is to predict the actual data as closely as possible.

- Cost function (A): a cost function is a function that for a whole series of inputs calculates the distance between the output of this input passed through the model function, and the actual result in the data point cloud. The sum of these distances is the result of our cost function, and is a measure of the accuracy of our model.

- Hyperparameters (B): the parameters that allow to manage the learning rate of an algorithm.

- Normalization (data science) (B): a change of benchmark used to rescaling the distances between the points of a data point cloud, so as to express the same relative information between these data points, but so that a specific algorithm has better results.

- Neural network (B): a neural network is an architecture using linear algebra to build a graph of computational cells (neurons) and run an optimization protocol (Darwinian mix, mix search for an optimum of a mathematical function) and allow an algorithm to become efficient for a given task in an unsupervised way.

- Linear regression (B): technique that allows to establish a model (here, an approximation of a cloud of data points) in the form of a vector subspace minimizing the distance to these points.

- Logistic regression (B): form of regression using the logistic function to obtain a probabilistic model of binary classification (true/false).

- Gradient descent (B): algorithm using an iterative gradient calculation to find an extremum of a complex mathematical function (typically, the minimization of a cost function).

- R-squared (B): one of the measures of the validity of a model: https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html



- Principal Components Analysis (C2): This is a dimensionality reduction technique (probably the best known and most widely used). Its principle is to express the underlying frame of reference of the data space in its most "expressive" form, i.e. that maximizes the variance of the data projected on the axes of the new frame of reference. This allows to have a maximum of information on the data cloud with a minimum of dimensions (axes). The origin of this new frame is the mean of the data points, the axes are computed by Singular Value Decomposition (a matrix simplification technique passing through the eigenvectors/eigenvalues) of the covariance matrix. Projecting the data points onto the 2-3 main axes allows to visualize an approximate but rather accurate version of a much more complex high dimensional data space. https://towardsdatascience.com/intuitive-understanding-of-eigenvectors-key-to-pca-a30a261c80de

- K-means clustering (C2): A classification technique that seeks to partition a group of n observations (an n-point data point cloud) into k "clusters" (sub-clusters; "cluster"), where each point belongs to the cluster with the closest mean. The problem solved by this algorithm is to find (relatively) efficiently (considering the complexity of the problem) a way to organize the points of the initial cloud, knowing that putting a point in a cluster rather than another one affects the average! The "classical" version of k-means clustering (called "naive k-means" or "Lloyd's algorithm"; not as efficient as other versions) separates the clusters according to hyperplanes and iterates by readjusting these hyperplanes in order to converge to a local optimum.

- K-nearest neighbors (B): algorithm for both classification and regression, assigning to each point either the category of its k nearest neighbors (classification), or the average of the values assigned to these k nearest neighbors (regression). It is an algorithm based on a choice of distance, and therefore for which the normalization of distances plays an important role.

- Support Vector Machines (C2): supervised learning algorithm for linear classification (i.e. partitioning of a statistical space by hyperplanes). Extensions of this algorithm exist for regression, and for non-linear classification (using kernel methods).

- Naive Bayes Classifiers (C2): family of algorithms based on conditional probabilities (working as hypotheses "if I am sure of this, then the probability that this is in such and such a category is...") to do classification.

- Decision Trees, Random forests (C2): machine learning techniques that build decision trees (conditional "if" trees) from the data; for classification and regression. These "if" trees are the original way of doing AI, before the rise of Deep Learning, and this technique allows to obtain these trees by machine learning rather than by human encoding. The advantage of decision trees is that they are very easy for a human being to read. https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956

- Curse of dimensionality (B): the larger the number of attributes (features, columns of a data table), the larger the number of data points must be to describe the space. The growth of the number of data points should ideally be to the power of the dimension (i.e. if we have 10 data points on a segment of length 1, then we will need 100 data points to have the same accuracy in a square of area 1, 1000 data points for the same accuracy in a cube of volume 1, etc.). This makes many algorithms inefficient for high dimensional spaces; and many problems very difficult to visualize.

- Dimensionality reduction (C2): a set of techniques used to reduce the dimension of spaces with too many attributes, with as little loss of information as possible in doing so. These techniques are used both to visualize high-dimensional statistical spaces, but also to make them tractable by different algorithms. https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/

- Garbage in, garbage out": a saying in data science and machine learning that says that an algorithm given rotten, biased, insufficient data, etc, will build a model whose predictions will be rotten, biased, insufficient, etc.

- Kernel Methods/Integral Kernel/Wrapping/Statistical Kernel/RKHS (C2/D): a set of methods that use function vector space covectors, or modified scalar products, to transform the shape of the data and obtain better results (e.g., by allowing approximate linear analysis of non-linear phenomena) https://en.wikipedia.org/wiki/Kernel_%28statistics%29 https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them

- Multiplayer Perceptron (MLP) (B): the simplest (fundamental, so-called "feedforward") neural network architecture. The nomenclature is sometimes ambiguous about what is an MLP or not, but basically an MLP takes a vector of data as input, passes it through layers (which are series of matrix multiplications applied to this vector, followed by a bias or a transformation of the value by a so-called "activation" function), and obtains a vector as output (which can be values limited to 0 and 1 for binary classification, between 0 and 1 for probabilities, or free real numbers). Learning takes place as for the vast majority of neural networks by the backpropagation algorithm (which compares the result vector to the expected vector, computes the cost for each coordinate, and does matrix multiplications by the transpose, a form of inverse matrix multiplication, in order to give each neuron of the network a value of its error responsibility, thus allowing it to modify its contribution to the value of the matrix that corresponds to it to better answer the given problem)

- Convolutional Neural Network (C2): neural network specialized in image processing, using convolution cells (in general, a `3 * 3` pixels square), in order to link groups of neighboring pixels, and to extract from them the shapes drawn by these pixels locally (vertical, horizontal, diagonal line, aligned bands, etc)

- Recurrent Neural Networks (C2): neural network architecture where "non-neighboring" layers can be linked in such a way as to affect each other (i.e. layer 1 that normally affects layer 2 can simultaneously affect layer 3; this technically makes layer 1 and layer 3 neighbors during feedforward; but not necessarily during backpropagation). There are two main classes of RNNs, those with finite impulse (which have no cycles, and can be expressed as a simple feedforward network) and those with infinite impulse (where "future" layers can affect "past" layers, thus creating cycles).

- Long Short Term Memory Network (C2): type of complex Recurrent Neural Network allowing to solve some of the biases of a naive approach, used for example in NLP and speech recognition. Their description is very technical.

- Generative Adversarial Networks (C2): neural network architecture, comprising two agents; one playing the role of forger, the other one of listener. Their cost function is increased by the performance of the other agent; both improve progressively by fighting against each other. Eventually, the forger can produce realistic data (image, sound, video, etc.), and the listener can distinguish and classify the true from the false efficiently.

- Q-learning (C2): Model-free reinforcement learning technique (does not need to know its environment: this technique allows the agent to understand its environment by itself and improve its decision process autonomously), widely used for AIs playing video games.


## Computer engineering

- Computer (C1): is both a Turing-complete mathematical model; and a physical electronic machine based on the Von Neumann architecture.

- CPU (C1): processor, fundamental calculation unit of a computer.

- GPU (C2): graphics card, computing unit specialized in parallelized computing. To caricature, a modern GPU is a simultaneous scalar product machine; this allows it to do linear algebra very efficiently.

- RAM (C1): memory unit, part of the active memory, containing the data and software currently running on the computer, usually also called RAM (in the form of bars).

- Read-only memory (C1): memory unit, part of the inactive memory, containing the inactive but saved data and executables of the machine, usually also called ROM (in the form of a rotating hard disk or flash memory).

- Motherboard (C1): control unit; contains most of the software that enables the computer to turn on, and serves as an interface to the various components.

- Power supply unit (C1): A unit that supplies power to the computer.



## Miscellaneous

- Cross-platform (C1): software or code that can run on several different computer architectures (eg: Mac OS, Windows, Linux-Debian, Linux-RedHat, iOS, Android, Raspberry Pi, Azure, etc)

- Version control / git / github (B): technology for managing the data of a project, usually computer. A repository contains all the archives and changes in the data of a project since its creation. Sites like github allow you to share your code projects online, and to retrieve the code of others.

- Reverse engineering (B): the act of studying the functioning of a product or an algorithm in order to understand how it was designed (diazed) and produced; in order to be able to reproduce it either as it is or by improving it.

- Divergent thinking: ability to think outside the box to broaden the scope of possible solutions to a design problem. If I tell you "you have to come up with at least 10 uses for a paper clip" or something like "you have to do something fun with a paper clip", you say "is it allowed to be 200 meters long and made of spongy foam, the paper clip?"

- Convergent thinking: the ability to filter possible solutions to a problem in a rigorous manner according to a set of constraints that must be well isolated and defined. It is similar to the mastery of the scientific/cartesian method.

- Design thinking: mental design methodology which uses both convergent thinking and divergent thinking.

- Algorithm design: activity of inventing protocols that automatically process data.
