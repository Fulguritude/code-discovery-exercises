{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description du notebook\n",
    "\n",
    "Ce notebook sert d'introduction à l'exploration de données. Un dataset petit et simple, contenu dans le fichier `cereal.csv`, décrit les apports nutritionnels de diverses marques de céréales. Un fichier csv est une façon assez épurée de représenter le contenu d'un tableau (ou \"spreadsheet\", un document dans le style Excel).\n",
    "\n",
    "On vous fournit un exemple assez approfondi de classe python, qui sert d'exemple de comment on construit un \"modèle\". En data science, un \"modèle\" est une façon mathématique de représenter un phénomène réel. Typiquement, on se sert de modèles pour extrapoler la donnée existante et prédire des éventualités futures.\n",
    "\n",
    "Par la suite, on demande d'utiliser les librairies Seaborn et Matplotlib pour faire de l'exploration et de la visualisation et de l'analyse exploratoire du dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lisez le code suivant, et essayez de le comprendre.\n",
    "\n",
    "Pour cela, faites un petit lexique des termes techiques (loss, linear regression, gradient, etc, tout ce que vous ne connaissez pas quoi).\n",
    "Regardez des vidéos youtube ou recherchez en ligne pour faire vos propres définitions explicatives des termes techniques (avec des mots, des images, etc).\n",
    "\n",
    "Bonus: essayez de le répliquer (une classe de régression linéaire par machine learnign), à votre sauce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install numpy scipy scikit-learn matplotlib seaborn pandas #data science/ML libraries\n",
    "\n",
    "#https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#sklearn.linear_model.LinearRegression #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn.linear_model.LinearRegression\n",
    "\n",
    "from time import time\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "start = time()\n",
    "\n",
    "def get_current_loadbar(i, length):\n",
    "\tpercent = i / length * 100\n",
    "\ts_percent = '%.0f' % percent\n",
    "\telapsed = time() - start\n",
    "\ts_elapsed = '%.2f' % elapsed\n",
    "\teta = (100 - percent) * elapsed / (percent + 0.0000001)\n",
    "\ts_eta = '%.2f' % eta\n",
    "\tres = \"ETA: \" + s_eta + \" [\" + s_percent.rjust(3, ' ') + \"%][\"\n",
    "\tbar = \">\".rjust(int((percent + 1) / (100 / 24)), '=') #percent + 1 looks nicer\n",
    "\t#24 columns for bar\n",
    "\tbar = bar.ljust(24, ' ')\n",
    "\tres = res + bar + \"] \" + str(i) + \"/\" + str(length) + \" | elapsed time \" + s_elapsed + \"s\"\n",
    "\treturn (res)\n",
    "\n",
    "\n",
    "def ft_progress(lst):\n",
    "\tlength = len(lst)\n",
    "\tfor i in lst:\n",
    "\t\tres = get_current_loadbar(i, length)\n",
    "\t\tyield res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    My personnal recoding of a linear regression class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, theta=[], alpha=0.001, n_cycle=100, n_epoch=10, verbose=False, learning_rate_type='constant'):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Generator of the class, initialize self.\n",
    "        Args:\n",
    "            theta: has to be a list or a numpy array, it is a vector of dimension (number of features + 1, 1).\n",
    "        Raises:\n",
    "            This method should not raise any Exception.\n",
    "        \"\"\"\n",
    "        self.theta = theta\n",
    "        self.alpha = alpha\n",
    "        self.n_cycle = n_cycle\n",
    "        self.n_epoch = n_epoch\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate_type = learning_rate_type # can be 'constant' or 'invscaling'\n",
    "        self.loss_list = []\n",
    "        self.alpha_list = []\n",
    "        self.crossval_loss_list = []\n",
    "\n",
    "\n",
    "    def set_base_theta_(self, X_train, X_cross, X_test, has_left_ones_column=False):\n",
    "        if has_left_ones_column:\n",
    "            self.theta = np.ones((X_train.shape[1]))\n",
    "        else:\n",
    "            self.theta = np.ones((X_train.shape[1] + 1))\n",
    "\n",
    "    def predict_(self, X):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Prediction of output using the hypothesis function (linear model).\n",
    "        Args:\n",
    "            theta: has to be a numpy.ndarray, a vector of dimension (number of features + 1, 1).\n",
    "            X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features).\n",
    "        Returns:\n",
    "            pred: numpy.ndarray, a vector of dimension (number of the training examples,1).\n",
    "            None if X does not match the dimension of theta.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if len(X) == 0 or len(X[0]) == 0 or len(self.theta) == 0:\n",
    "            #print (\"predict error: \" + str(len(X) == 0) + \" \" + str(len(X[0]) == 0) + \" \" + str(len(X[0]) != len(self.theta) - 1))\n",
    "            return None\n",
    "        if len(X[0]) + 1 == len(self.theta):\n",
    "            return np.dot(X, self.theta[1:]) + self.theta[0]\n",
    "        if len(X[0]) == len(self.theta):\n",
    "            return np.dot(X, self.theta)\n",
    "        return None\n",
    "\n",
    "    def loss_elems_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Calculates all the elements 0.5*M*(y_pred - y)^2 of the cost function.\n",
    "        Args:\n",
    "            theta: has to be a numpy.ndarray, a vector of dimension (number of features + 1, 1).\n",
    "            X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features).\n",
    "            Y: has to be a numpy.ndarray, a matrix of dimensions (number of training examples, 1)\n",
    "        Returns:\n",
    "            J_elem: numpy.ndarray, a vector of dimension (number of the training examples,1).\n",
    "            None if there is a dimension matching problem between X, Y or theta.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        return 0.5 * ((self.predict_(X) - Y) ** 2) / len(Y)\n",
    "\n",
    "    def loss_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Calculates the value of cost function.\n",
    "        Args:\n",
    "            theta: has to be a numpy.ndarray, a vector of dimension (number of features + 1, 1).\n",
    "            X: has to be a numpy.ndarray, a vector of dimension (number of training examples, number of features).\n",
    "            Y: has to be a numpy.ndarray, a matrix of dimensions (number of training examples, 1)\n",
    "        Returns:\n",
    "            J_value : has to be a float.\n",
    "            None if X does not match the dimension of theta.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        return np.sum(self.loss_elems_(X, Y))\n",
    "\n",
    "    def gradient_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Computes a gradient vector. The two arrays must have the compatible dimensions.\n",
    "        NB: this function get the gradient by minimizing the error as much as possible\n",
    "        Args:\n",
    "            theta: has to be a numpy.ndarray, a vector of dimension n.\n",
    "            X: has to be a numpy.ndarray, a matrix of dimension m * n.\n",
    "            Y: has to be a numpy.ndarray, a vector of dimension m.\n",
    "        Returns:\n",
    "            The gradient as a numpy.ndarray, a vector of dimensions n * 1.\n",
    "            None if x, y, or theta are empty numpy.ndarray.\n",
    "            None if x, y and theta do not have compatible dimensions.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            #print(str(len(X) == 0) + \" \" + str(len(X[0]) == 0) + \" \" + str(len(Y) == 0) + \" \" + str(len(self.theta) == 0 or len(X) != len(Y)) + \" \" + str(len(X[0]) != len(self.theta) - 1) + \" \" + str(len(Y[0]) != 1))\n",
    "            return None\n",
    "        Y_hat = self.predict_(X)\n",
    "        loss_vec = (Y_hat - Y)\n",
    "        gradient = np.dot(X.T, loss_vec)\n",
    "        if len(X[0]) + 1 == len(self.theta):\n",
    "            tmp = np.zeros((len(gradient) + 1))\n",
    "            tmp[0] = sum(loss_vec)\n",
    "            tmp[1:] = gradient\n",
    "            gradient = tmp\n",
    "        inv_m = 1 / len(loss_vec)\n",
    "        return gradient * inv_m\n",
    "\n",
    "    def fit_(self, X, Y, show_progress=False):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Performs a fit of Y(output) with respect to X.\n",
    "        Args:\n",
    "            theta: has to be a numpy.ndarray, a vector of dimension (number of features + 1, 1).\n",
    "            X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features).\n",
    "            Y: has to be a numpy.ndarray, a vector of dimension (number of training examples, 1).\n",
    "        Returns:\n",
    "            new_theta: numpy.ndarray, a vector of dimension (number of the features +1, 1).\n",
    "            None if there is a matching dimension problem.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        for cycle in range(self.n_cycle):\n",
    "            if show_progress:\n",
    "                print(get_current_loadbar(cycle + 1, self.n_cycle), end=(\"\\n\" if cycle + 1 == self.n_cycle else \"\\r\"))\n",
    "            gradient = self.gradient_(X, Y)\n",
    "            self.theta = self.theta - self.alpha * gradient \n",
    "        return self.theta\n",
    "\n",
    "#    def fit_data_(self, data, x_axis_key, y_axis_key):\n",
    "#        X = np.array(data[x_axis_key])\n",
    "#        Y = np.array(data[y_axis_key])\n",
    "#        if self.verbose:\n",
    "#            print(\"Fitting model... \\ntheta: \" + str(self.theta) + \"\\nX:\" + str(X) + \"\\nY:\" + str(Y))\n",
    "#        self.fit_(X, Y, self.alpha, self.n_cycle)\n",
    "#\n",
    "#    def fit_data_multilinear_(self, data, x_axis_keys, y_axis_key):\n",
    "#        X = np.array(data[x_axis_keys])\n",
    "#        Y = np.array(data[[y_axis_key]])\n",
    "#        print(\"Fitting \" + str(x_axis_keys) + \" against \" + y_axis_key)\n",
    "#        print(\"MSE before fit: \" + str(self.mse_(X, Y)))\n",
    "#        self.fit_(X, Y, self.alpha, self.n_cycle, True)\n",
    "#        print(\"MSE after fit: \" + str(self.mse_(X, Y)))\n",
    "\n",
    "    def mse_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Computes the mean squared error of three non-empty numpy.ndarray,\n",
    "            without any for-loop. The three arrays must have compatible dimensions.\n",
    "        Args:\n",
    "            y: has to be an numpy.ndarray, a vector of dimension m * 1.\n",
    "            x: has to be an numpy.ndarray, a matrix of dimesion m * n.\n",
    "            theta: has to be an numpy.ndarray, a vector of dimension n * 1.\n",
    "        Returns:\n",
    "            The mean squared error as a float.\n",
    "            None if y, x, or theta are empty numpy.ndarray.\n",
    "            None if y, x or theta does not share compatibles dimensions.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        Y_hat = self.predict_(X)\n",
    "        loss_vec = Y_hat - Y\n",
    "        return np.dot(loss_vec.T, loss_vec) / len(loss_vec)\n",
    "\n",
    "    def mae_(self, X, Y):\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        Y_hat = self.predict_(X)\n",
    "        loss_vec = Y_hat - Y\n",
    "        return sum(math.abs(loss_vec)) / len(loss_vec)\n",
    "\n",
    "    def rmse_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Calculate the RMSE between the predicted output and the real output.\n",
    "        Args:\n",
    "            X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features).\n",
    "            Y: has to be a numpy.ndarray, a vector of dimension (number of training examples, 1).\n",
    "        Returns:\n",
    "            rmse: has to be a float.\n",
    "            None if there is a matching dimension problem.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        return math.sqrt(self.mse_(X, Y))\n",
    "\n",
    "    def r2score_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Calculate the R2score between the predicted output and the output.\n",
    "            Best possible score is 1.0; bad scores are near 0, or below\n",
    "        Args:\n",
    "            X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features).\n",
    "            Y: has to be a numpy.ndarray, a vector of dimension (number of training examples, 1).\n",
    "        Returns:\n",
    "            r2score: has to be a float.\n",
    "            None if there is a matching dimension problem.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        Y_mean = np.mean(Y)\n",
    "        total_sum_squares = np.sum((Y - Y_mean) ** 2)\n",
    "        Y_hat = self.predict_(X)\n",
    "        residual_sum_squares = np.sum((Y_hat - Y) ** 2)\n",
    "        return 1 - residual_sum_squares / total_sum_squares\n",
    "\n",
    "#    def score_(self, x, y_true):\n",
    "#        \"\"\"\n",
    "#        Returns the mean accuracy on the given test data and labels.\n",
    "#        Arg:\n",
    "#            x: a 1d or 2d numpy ndarray for the samples\n",
    "#            y: a scalar or a numpy ndarray for the correct labels\n",
    "#        Returns:\n",
    "#            Mean accuracy of self.predict(x_train) with respect to y_true\n",
    "#            None on any error.\n",
    "#        Raises:\n",
    "#            This method should not raise any Exception.\n",
    "#        \"\"\"\n",
    "#        y_pred = self.predict_class_(x)        \n",
    "#        if len(y_pred) != len(y_true):\n",
    "#            return None\n",
    "#        #print(\"y_pred.shape: \" + str(y_pred.shape) + \"\\ny_true.shape: \" + str(y_pred.shape))\n",
    "#        return (y_pred == y_true).mean()\n",
    "\n",
    "    def training_setup_(self, X_train, X_cross, X_test):\n",
    "        print(\"\\nSetup for training of linear model...\")\n",
    "        self.set_base_theta_(X_train, X_cross, X_test)\n",
    "        #diagnostics:\n",
    "        for i in range(len(X_train[0])):\n",
    "            column = X_train[:, i]\n",
    "            print(\"X_train column \" + str(i) + \":\\n\")\n",
    "            print(\"\\t- mode    : \" + str(stats.mode(column)))\n",
    "            print(\"\\t- mean    : \" + str(np.mean(column)))\n",
    "            print(\"\\t- median  : \" + str(np.median(column)))\n",
    "            print(\"\\t- variance: \" + str(np.var(column)))\n",
    "            #print(\"\\t- stddev  : \" + str(stats.stddev(column)))\n",
    "            print(\"\\t- min     : \" + str(np.min(column)))\n",
    "            print(\"\\t- max     : \" + str(np.max(column)))\n",
    "            #print(\"\\t- range   : \" + str(stats.range(column)))\n",
    "\n",
    "            column = X_cross[:, i]\n",
    "            print(\"X_cross column \" + str(i) + \":\\n\")\n",
    "            print(\"\\t- mode    : \" + str(stats.mode(column)))\n",
    "            print(\"\\t- mean    : \" + str(np.mean(column)))\n",
    "            print(\"\\t- median  : \" + str(np.median(column)))\n",
    "            print(\"\\t- variance: \" + str(np.var(column)))\n",
    "            #print(\"\\t- stddev  : \" + str(stats.stddev(column)))\n",
    "            print(\"\\t- min     : \" + str(np.min(column)))\n",
    "            print(\"\\t- max     : \" + str(np.max(column)))\n",
    "            #print(\"\\t- range   : \" + str(stats.range(column)))\n",
    "\n",
    "            column = X_test [:, i]\n",
    "            print(\"X_test  column \" + str(i) + \":\\n\")\n",
    "            print(\"\\t- mode    : \" + str(stats.mode(column)))\n",
    "            print(\"\\t- mean    : \" + str(np.mean(column)))\n",
    "            print(\"\\t- median  : \" + str(np.median(column)))\n",
    "            print(\"\\t- variance: \" + str(np.var(column)))\n",
    "            #print(\"\\t- stddev  : \" + str(stats.stddev(column)))\n",
    "            print(\"\\t- min     : \" + str(np.min(column)))\n",
    "            print(\"\\t- max     : \" + str(np.max(column)))\n",
    "            #print(\"\\t- range   : \" + str(stats.range(column)))\n",
    "\n",
    "    def training_handle_epoch_(self, X, Y, epoch, show_progress=False):\n",
    "        new_loss = self.loss_(X, Y)\n",
    "        self.loss_list.append(new_loss)\n",
    "        self.fit_(X, Y, show_progress)\n",
    "        #print(\"theta: \" + str(self.theta))\n",
    "        if self.learning_rate_type == 'invscaling':\n",
    "            new_alpha = 0.1 / new_loss if new_loss != 0 else self.alpha\n",
    "            self.alpha = new_alpha if new_alpha != 0 and self.alpha != 0 else 0.000001\n",
    "            self.alpha_list.append(new_alpha)\n",
    "        if self.verbose:\n",
    "            print(\"epoch \" + str(epoch) + \": \\tloss = \" + str(self.loss_list[-1]))\n",
    "\n",
    "    def train_(self, X_train, Y_train, X_cross, Y_cross, X_test, Y_test, show_progress=False, show_hyperparameter_stats=False):\n",
    "        #TODO dimension checks\n",
    "        self.training_setup_(X_train, X_cross, X_test)\n",
    "        #print(\"testing..\")\n",
    "        #print(X)\n",
    "        #print(Y)\n",
    "        #print(self.theta)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            self.crossval_loss_list = self.crossval_loss_list + [self.loss_(X_cross, Y_cross)]\n",
    "            self.training_handle_epoch_(X_train, Y_train, epoch, show_progress)\n",
    "        print(\"Score on training dataset:\\trmse: \" + str(self.rmse_(X_train, Y_train)) + \"\\t| r2_score: \" + str(self.r2score_(X_train, Y_train)))\n",
    "        print(\"Score on crossval dataset:\\trmse: \" + str(self.rmse_(X_cross, Y_cross)) + \"\\t| r2_score: \" + str(self.r2score_(X_cross, Y_cross)))\n",
    "        print(\"Score on test     dataset:\\trmse: \" + str(self.rmse_(X_test , Y_test )) + \"\\t| r2_score: \" + str(self.r2score_(X_test , Y_test )))\n",
    "        print(\"Final theta: \" + str(self.theta) + \"\\t| norm: \" + str(np.linalg.norm(self.theta)) + \"\\t| average spectral dist: \" + str(np.linalg.norm(self.theta) / len(self.theta)))\n",
    "        if show_hyperparameter_stats:\n",
    "            self.plot_learning_()\n",
    "\n",
    "    def plot_data_(self, X, Y, fig, int_code, label, axis1_label=\"x1\", axis2_label=\"x2\", axis3_label=\"y\"):\n",
    "        ax = fig.add_subplot(int_code, projection=\"3d\", label=label)\n",
    "        X_1 = X.T[0]\n",
    "        X_2 = X.T[1]\n",
    "        ax.scatter(X_1, X_2, Y, c='r', marker='o')\n",
    "        ax.set_xlabel(axis1_label)\n",
    "        ax.set_ylabel(axis2_label)\n",
    "        ax.set_zlabel(axis3_label)\n",
    "\n",
    "    def plot_data_all_(self, X_train, Y_train, X_cross, Y_cross, X_test, Y_test, axis1_label=\"x1\", axis2_label=\"x2\", axis3_label=\"y\"):\n",
    "        fig = mpl.figure()\n",
    "        self.plot_data_(X_train, Y_train, fig, 131, \"train\", axis1_label=axis1_label, axis2_label=axis2_label, axis3_label=axis3_label)\n",
    "        self.plot_data_(X_cross, Y_cross, fig, 132, \"cross\", axis1_label=axis1_label, axis2_label=axis2_label, axis3_label=axis3_label)\n",
    "        self.plot_data_(X_test , Y_test , fig, 133, \"test \", axis1_label=axis1_label, axis2_label=axis2_label, axis3_label=axis3_label)\n",
    "        mpl.show()\n",
    "\n",
    "    def plot_learning_(self):\n",
    "        fig = mpl.figure()\n",
    "        ax = fig.add_subplot(311)\n",
    "        ax.plot(list(range(self.n_epoch)), self.loss_list, color=\"red\", linewidth=3, label=\"Loss(epoch)\")\n",
    "        if self.learning_rate_type == \"invscaling\":\n",
    "            ay = fig.add_subplot(312)\n",
    "            ay.plot(list(range(self.n_epoch)), self.alpha_list, color=\"green\", linewidth=3, label=\"Alpha(epoch)\")\n",
    "        az = fig.add_subplot(313)\n",
    "        az.plot(list(range(self.n_epoch)), self.crossval_loss_list, color=\"purple\", linewidth=3, label=\"CrossValLoss(epoch)\")\n",
    "        mpl.show()\n",
    "\n",
    "    def plot_model_(self, data, x_axis_key, y_axis_key, point_color=\"lightblue\", line_color=\"green\"):\n",
    "        X   = np.array(data[x_axis_key])\n",
    "        Y   = np.array(data[y_axis_key])\n",
    "        fig = mpl.figure()\n",
    "        ax  = fig.add_subplot(111)\n",
    "        ax.set_xlim(min(X) - 1, max(X) + 1)\n",
    "        ax.set_ylim(min(Y) - 1, max(Y) + 1)\n",
    "        ax.scatter(X, Y, color=point_color)\n",
    "        X_hat = np.arange(min(X), max(X), (max(X) - min(X)) / 200)\n",
    "        X_hat = X_hat.reshape((len(X_hat), 1))\n",
    "        Y_hat = self.predict_(X_hat)\n",
    "        #print(\"X_hat: \" + str(X_hat) + \"\\nY_hat:\" + str(Y_hat) + \"\\n\")\n",
    "        ax.plot(X_hat, Y_hat, color = line_color, linewidth=3)\n",
    "        mpl.show()\n",
    "\n",
    "    def plot_model_multilinear_(self, data, x_axis_keys, shown_x_axis_key, y_axis_key, base_color=\"lightblue\", prediction_color=\"green\"):\n",
    "        X       = np.array(data[x_axis_keys])\n",
    "        X_shown = np.array(data[[shown_x_axis_key]])\n",
    "        Y       = np.array(data[[y_axis_key]])\n",
    "        fig     = mpl.figure()\n",
    "        ax      = fig.add_subplot(111)\n",
    "        ax.set_xlim(min(X_shown) - 10, max(X_shown) + 10)\n",
    "        ax.set_ylim(min(Y) - 100, max(Y) + 100)\n",
    "        ax.scatter(X_shown, Y, color = base_color)\n",
    "        Y_hat   = self.predict_(X)\n",
    "        ax.scatter(X_shown, Y_hat, color = prediction_color)\n",
    "        mpl.show()\n",
    "\n",
    "    def normalequation_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Also called Ordinary Least Squares.\n",
    "            (xT x)^-1 xT is called the Moore-Penrose inverse of a matrix\n",
    "            Perform the normal equation to get the theta parameters of the hypothesis h and stock them in self.theta.\n",
    "        Args:\n",
    "            X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features)\n",
    "            Y: has to be a numpy.ndarray, a vector of dimension (number of training examples,1)\n",
    "        Returns:\n",
    "            Returns self.theta\n",
    "        Raises:\n",
    "            This method should not raise any Exceptions.\n",
    "        \"\"\"        \n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        X_trans = X.T\n",
    "        #print(np.dot(X_trans, X))\n",
    "        inv_xT_x = np.linalg.inv(np.dot(X_trans, X))\n",
    "        xT_y = np.dot(X_trans, Y)\n",
    "        #print(self.theta)\n",
    "        self.theta = np.dot(inv_xT_x, xT_y)\n",
    "        #print(self.theta)\n",
    "        return self.theta\n",
    "\n",
    "    def normalequation_data_(self, data, x_axis_keys, y_axis_key):\n",
    "        X = np.array(data[x_axis_keys])\n",
    "        Y = np.array(data[[y_axis_key]])\n",
    "        print(\"Fitting \" + str(x_axis_keys) + \" against \" + y_axis_key)\n",
    "        print(\"MSE before normalequation: \" + str(self.mse_(X, Y)))\n",
    "        self.normalequation_(X, Y)\n",
    "        print(\"New theta:\" + str(self.theta))\n",
    "        print(\"MSE after normalequation: \" + str(self.mse_(X, Y)))\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegressionRidge(LinearRegression):\n",
    "\n",
    "    def __init__(self, theta=[], alpha=0.001, n_cycle=100, n_epoch=10, verbose=False, learning_rate_type='constant', lambda_=0.01):\n",
    "        super().__init__(theta, alpha, n_cycle, n_epoch, verbose, learning_rate_type)\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def loss_elems_(self, X, Y):\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        if len(X[0]) == len(self.theta):\n",
    "            return super().loss_elems_(X, Y) + self.lambda_ * 0.5 * np.dot(self.theta.T, self.theta) / len(Y)\n",
    "        if len(X[0]) + 1 == len(self.theta):\n",
    "            return super().loss_elems_(X, Y) + self.lambda_ * 0.5 * np.dot(self.theta[1:].T, self.theta[1:]) / len(Y)\n",
    "\n",
    "    def regularized_gradient_(self, X, Y):\n",
    "        \"\"\"\n",
    "        Computes the regularized linear gradient of three non-empty numpy.ndarray, with two for-loop. The three arrays must have compatible dimensions.\n",
    "        Args:\n",
    "            y: has to be a numpy.ndarray, a vector of dimension m * 1.\n",
    "            x: has to be a numpy.ndarray, a matrix of dimesion m * n.\n",
    "            theta: has to be a numpy.ndarray, a vector of dimension n * 1. \n",
    "            lambda_: has to be a float.\n",
    "        Returns:\n",
    "            A numpy.ndarray, a vector of dimension n * 1, containing the results of the formula for all j.\n",
    "            None if y, x, or theta are empty numpy.ndarray.\n",
    "            None if y, x or theta does not share compatibles dimensions.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        gradient = super().gradient_(X, Y)\n",
    "        reg_param = (self.lambda_ * self.theta) / len(Y)\n",
    "        return gradient + reg_param\n",
    "\n",
    "    def fit_(self, X, Y, show_progress=False):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Performs a fit of Y(output) with respect to X.\n",
    "        Args:\n",
    "            theta: has to be a numpy.ndarray, a vector of dimension (number of features + 1, 1).\n",
    "            X: has to be a numpy.ndarray, a matrix of dimension (number of training examples, number of features).\n",
    "            Y: has to be a numpy.ndarray, a vector of dimension (number of training examples, 1).\n",
    "        Returns:\n",
    "            new_theta: numpy.ndarray, a vector of dimension (number of the features +1, 1).\n",
    "            None if there is a matching dimension problem.\n",
    "        Raises:\n",
    "            This function should not raise any Exception.\n",
    "        \"\"\"\n",
    "        if (len(X) == 0 or len(X[0]) == 0 or len(Y) == 0 or len(self.theta) == 0 or len(X) != len(Y)\n",
    "            or (len(X[0]) != len(self.theta) and len(X[0]) + 1 != len(self.theta))):\n",
    "            return None\n",
    "        for cycle in range(self.n_cycle):\n",
    "            if show_progress:\n",
    "                print(get_current_loadbar(cycle + 1, self.n_cycle), end=(\"\\n\" if cycle + 1 == self.n_cycle else \"\\r\"))\n",
    "            gradient = self.regularized_gradient_(X, Y)\n",
    "            self.theta = self.theta - self.alpha * gradient\n",
    "        return self.theta\n",
    "\n",
    "    def training_setup_(self, X_train, X_cross, X_test):\n",
    "        print(\"\\nSetup for training of ridge linear model...\")\n",
    "        self.set_base_theta_(X_train, X_cross, X_test)\n",
    "\n",
    "#    def training_handle_epoch_(self, X, Y, epoch, show_progress=False):\n",
    "#        super().training_handle_epoch_(X, Y, epoch, show_progress)\n",
    "\n",
    "    def plot_ridge_trace_(self, X, Y, show_progress=False):\n",
    "        theta_list = []\n",
    "        lambda_list = []\n",
    "        final_loss_list = []\n",
    "        for i in range(-5, 7):\n",
    "            self.lambda_ = 0.5 * 10 ** i\n",
    "            lambda_list = lambda_list + [self.lambda_]\n",
    "            print(\"\\nTraining ridge model for lambda = \" + str(self.lambda_))\n",
    "            self.train_(X, Y, X, Y, X, Y, show_progress)\n",
    "            theta_list.append(self.theta)\n",
    "            final_loss_list.append(self.loss_list[-1])\n",
    "        fig = mpl.figure()\n",
    "\n",
    "        #ridge trace\n",
    "        ax = fig.add_subplot(211)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlim(lambda_list[0] / 10, lambda_list[-1] * 10)\n",
    "        ##ax.set_ylim(-5, 5)\n",
    "        #print(np.array(theta_list))\n",
    "        #print(lambda_list)\n",
    "        theta_mat = np.array(theta_list).T\n",
    "        #print(theta_mat)\n",
    "        for i in range(len(theta_mat)):\n",
    "            theta_coef_evolution = theta_mat[i]\n",
    "            line_color = i * 20 / 255\n",
    "            line_color = 0.75 if i > 0.75 else (0. if i < 0. else i)\n",
    "            line_color = (line_color, line_color, line_color)\n",
    "            ax.plot(lambda_list, theta_coef_evolution, color=line_color, lw=3, label=\"Line theta_\" + str(i))\n",
    "\n",
    "        #performance\n",
    "        ay = fig.add_subplot(212)\n",
    "        ay.set_xscale('log')\n",
    "        ay.set_xlim(lambda_list[0] / 10, lambda_list[-1] * 10)\n",
    "        ay.plot(lambda_list, final_loss_list, color=\"red\", lw=3, label=\"FinalLoss(lambda)\")\n",
    "\n",
    "        mpl.legend()\n",
    "        mpl.show()\n",
    "\n",
    "    #def train_(self, X, Y, X_test, Y_test, show_progress=False, show_hyperparameter_stats=False):\n",
    "\n",
    "\n",
    "\n",
    "class PolynomialRegression(LinearRegression):\n",
    "\n",
    "    def __init__(self, theta=[], alpha=0.001, n_cycle=100, n_epoch=10, verbose=False, learning_rate_type='constant', degree=3):\n",
    "        super().__init__(theta, alpha, n_cycle, n_epoch, verbose, learning_rate_type)\n",
    "        self.degree = degree\n",
    "        self.X_train = np.zeros((1,1))\n",
    "        self.X_test = np.zeros((1,1))\n",
    "\n",
    "    def set_base_theta_(self, X_train, X_cross, X_test, has_left_ones_column=False):\n",
    "        if has_left_ones_column:\n",
    "            param_amount = X_train.shape[1] - 1\n",
    "            self.theta = np.ones((param_amount * self.degree + 1))\n",
    "            self.X_train = np.zeros((len(X_train), len(self.theta)))\n",
    "            self.X_cross = np.zeros((len(X_cross), len(self.theta)))\n",
    "            self.X_test  = np.zeros((len(X_test ), len(self.theta)))\n",
    "            self.X_train[:, :param_amount + 1] = X_train\n",
    "            self.X_cross[:, :param_amount + 1] = X_cross\n",
    "            self.X_test [:, :param_amount + 1] = X_test \n",
    "            for i in range(1, self.degree):\n",
    "                self.X_train[:, 1 + param_amount * i: 1 + param_amount * (i + 1)] = self.X_train[:, 1:1 + param_amount] ** (i + 1)\n",
    "                self.X_cross[:, 1 + param_amount * i: 1 + param_amount * (i + 1)] = self.X_cross[:, 1:1 + param_amount] ** (i + 1)\n",
    "                self.X_test [:, 1 + param_amount * i: 1 + param_amount * (i + 1)] = self.X_test [:, 1:1 + param_amount] ** (i + 1)\n",
    "        else:\n",
    "            param_amount = X_train.shape[1]\n",
    "            self.theta = np.ones((param_amount * self.degree + 1))\n",
    "            self.X_train = np.zeros((len(X_train), len(self.theta)))\n",
    "            self.X_cross = np.zeros((len(X_cross), len(self.theta)))\n",
    "            self.X_test  = np.zeros((len(X_test ), len(self.theta)))\n",
    "            self.X_train[:, 0] = np.ones(len(X_train))\n",
    "            self.X_cross[:, 0] = np.ones(len(X_cross))\n",
    "            self.X_test [:, 0] = np.ones(len(X_test ))\n",
    "            self.X_train[:, 1:param_amount + 1] = X_train\n",
    "            self.X_cross[:, 1:param_amount + 1] = X_cross\n",
    "            self.X_test [:, 1:param_amount + 1] = X_test\n",
    "            for i in range(1, self.degree):\n",
    "                self.X_train[:, 1 + param_amount * i : 1 + param_amount * (i + 1)] = self.X_train[:, 1:1 + param_amount] ** (i + 1)\n",
    "                self.X_cross[:, 1 + param_amount * i : 1 + param_amount * (i + 1)] = self.X_cross[:, 1:1 + param_amount] ** (i + 1)\n",
    "                self.X_test [:, 1 + param_amount * i : 1 + param_amount * (i + 1)] = self.X_test [:, 1:1 + param_amount] ** (i + 1)\n",
    "            \n",
    "            #self.X_train = self.X_train[:, [0, 1, 2, 4, 5]]\n",
    "            #self.X_cross = self.X_cross[:, [0, 1, 2, 4, 5]]\n",
    "            #self.X_test  = self.X_test [:, [0, 1, 2, 4, 5]]\n",
    "            #self.theta = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "    def training_setup_(self, X_train, X_cross, X_test):\n",
    "        print(\"\\nSetup for training of polynomial model...\")\n",
    "        self.set_base_theta_(X_train, X_cross, X_test)\n",
    "\n",
    "    def train_(self, X_train, Y_train, X_cross, Y_cross, X_test, Y_test, show_progress=False, show_hyperparameter_stats=False):\n",
    "        self.training_setup_(X_train, X_cross, X_test)\n",
    "        X_train = self.X_train\n",
    "        X_cross = self.X_cross\n",
    "        X_test  = self.X_test \n",
    "        for epoch in range(self.n_epoch):\n",
    "            self.crossval_loss_list = self.crossval_loss_list + [self.loss_(X_cross, Y_cross)]\n",
    "            self.training_handle_epoch_(X_train, Y_train, epoch, show_progress)\n",
    "        print(\"Score on training dataset:\\trmse: \" + str(self.rmse_(X_train, Y_train)) + \"\\t| r2_score: \" + str(self.r2score_(X_train, Y_train)))\n",
    "        print(\"Score on crossval dataset:\\trmse: \" + str(self.rmse_(X_cross, Y_cross)) + \"\\t| r2_score: \" + str(self.r2score_(X_cross, Y_cross)))\n",
    "        print(\"Score on test     dataset:\\trmse: \" + str(self.rmse_(X_test , Y_test )) + \"\\t| r2_score: \" + str(self.r2score_(X_test , Y_test )))\n",
    "        print(\"Final theta: \" + str(self.theta) + \"\\t| norm: \" + str(np.linalg.norm(self.theta)) + \"\\t| average spectral dist: \" + str(np.linalg.norm(self.theta) / len(self.theta)))\n",
    "        if show_hyperparameter_stats:\n",
    "            self.plot_learning_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisez la librairie pandas pour lire le fichier csv fourni et charger son contenu dans une dataframe, puis affichez un overview (survol) de son contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduisez la dataframe pour ne garder que les colonnes qui contiennent des valeurs numériques uniquement. Affichez un overview de la frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouvez et utilisez la méthode (=fonction) appropriée de la classe dataframe de pandas pour faire un résumé des statistiques basiques du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisez la librairie seaborn pour faire une représentation graphique 2D du croisement de deux colonnes, au choix, de la frame. Variez les deux colonnes choisies afin d'explorer les relations (corrélation ou non) des différentes colonnes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produisez les diagrammes à moustache (boxplot) de toutes les colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produisez les diagrammes à moustache de 5 colonnes seulement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produisez un histogramme de l'une des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produire un diagramme d'une estimation par noyau (kernel density estimation plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produire un diagramme croisant deux colonnes, de manière à la fois univariée et bivariée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisez le dataset en 3 sous-groupe: entraînement (60% des échantillons), test (20%) et validation (20%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentez pour chacun de ces groupes 3 colonnes dans un repère 3D avec matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînez un des modèles fournis ci-dessus sur la donnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fournissez des métriques de qualité du modèle; et comparez les à ceux de l'équation normale (normalequation_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
