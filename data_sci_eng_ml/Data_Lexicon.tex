\documentclass{article}

\title{Lexicon for Math, CS, Data & ML}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{derivative}

\begin{document}


\section*{Lexicon}

The following is large, thematic lexicon, of various terms relating to the domain of "data". It is mostly oriented to individuals trying to build or improve a technical profile in the domain. However, it was designed to also be of interest to anyone that might need to work with technical data profiles, even if they themselves might not be one (business, HR, legal, designers, etc).

It includes vocabulary from mathematics (everything from combinatorics to topological data analysis), computer science (programming paradigms to systems engineering), machine learning, professions in data science, etc.




\subsection*{Code for the level of various terms}

Below is a code to understand the relative difficulty or importance of various terms in this lexicon. Note that even if a term is "above your level of understanding or ambition", it might still be useful to have heard of it. Also note that some terms might be fundamental for one profile, but only necessary later on for others (typically, all of linear algebra); this will be described through letter combinations.

For example "(AM, BE)" would mean fundamental to mathematicians, but intermediate-level for engineers. "A", alone, would mean fundamental for everyone, in terms of general culture or being an informed citizen; "AMELT" would mean fundamental for all technical profiles; while "AMELTO" would mean fundamental to anyone required to communicate regularly with a technical profiles, even if they are not technical themselves.

Difficulty / importance code:

\begin{itemize}
	\item \textbf{A}: fundamental or easy, to be known and understood for any kind of work profile in data that you're trying to build or improve (even non-technical ones).
	\item \textbf{B}: important or intermediate, to be understood when you start to attempt productive work in the domain, or needed to follow basic conversations with colleagues about their domain.
	\item \textbf{C}: advanced or difficult, to be understood by those trying to build a specialist profile.
	\item \textbf{D}: very advanced or very difficult, to be understood by those trying to build an expert profile.
\end{itemize}

Thematic code:

\begin{itemize}
	\item \textbf{M (math)}: applies to profiles which rely on mathematics a lot (data science, statistics, ML engineering, ML research, algorithmics research, etc).
	\item \textbf{E (engineer)}: applies to profiles which rely on computer engineering or software engineering a lot (systems engineering, network engineering, distributed computing, etc).
	\item \textbf{L (learning)}: applies to profiles which rely on machine learning, algorithmics and statistics, and their specific techniques, outside of general mathematics.
	\item \textbf{T (technical)}: applies to other profiles which may have an interest in this document, and have a technical profile (physical modelling, financial analysis, business analysis, etc).
	\item \textbf{O (other)}: applies to any other profiles which may have an interest in this document (designers, illustrators, data collectors, lawyers, businesspeople, etc), but do not themselves have a technical profile (though they may need to communicate with such people).
	\item \textbf{G (general)}: interesting to know, but peripheral to the subject of data (general culture).
\end{itemize}




\subsection*{Data generalities}

\subsubsection*{Data generalities: fundamentals}

\begin{itemize}
	\item \textbf{Data (A)}: "data" describes any form of raw information about the world (real or virtual) that can be measured, collected, described, transmitted, organized, transformed, modelled, and/or analyzed. The human goal for "data" is to turn it into "analysis" (semantic information, information in the "non-raw" sense), in order to aid human decision-making by improving our understanding the dynamics of our world and its various component systems.

	\item \textbf{Domain, business domain (A)}: a "domain" is a field of human activity, generally one of business activity. This includes everything from the management of shipping ports, to mariage counseling, to meteorological modeling. "Domain knowledge" refers to the ways that people that are active in this domain tend to describe it; this includes all the vocabulary, processes, or mental models specific to actors of that domain.

	\item \textbf{System (A)}: a "system" is collection of "\textbf{entities}" (or "agents", or "nodes") and the "\textbf{interactions}" (or "links", or "edges") between these entities. In a system, you generally have various collections of entities which can be grouped together (because they are of the same "type": while distinct, they are analogous to each other, like "atoms", or "humans"). Different kinds of interactions exist between each other based on their respective types, though generally, interactions between elements of type A and elements of type B can be described by a common "interaction type" A->B (like "covalent bonds" between atoms, "relationships" between humans, or "ownership" of an object by a human). "Systems engineering" in particular, generally refers to the specific case of information systems / computer systems, and the network interactions between these computers, or their interfaces with their users.

	\item \textbf{Model (A)}: a "model" is a mathematical way of describing some entity, or set of entities, or domain; within the real or virtual world. Nowadays, all models (which used to be mostly abstract tools) tend to become implemented into computers somehow. Models insist in different ways on different mathematics. Some are specialized for statistical analysis and/or predictions, some are specialized to describe how an information system is meant to be designed, some are specialized in describing existing non-information (such as physical) systems, etc. Generally, when building an overall system for a specific domain, there are various types of models which are required, and these need to be associated, designed in common, or linked together for the overall domain to be modeled properly.

	\item \textbf{Big data (A)}: this term is somewhat hard to define cleanly, and is used more as a marketing buzzword, rather than having an actual technical definition. After all, how "big" is "big" ? Is a spreadsheet with 10 millions columns "big" ? Analyzing it would have required dozens of computers some decades ago; nowadays a single average laptop can typically handle it. We can perhaps divide the expression into its two subterms: big and data. "Big" would then generally refers to "big enough to require a large-scale information (computer) system, and/or algorithms that are used to handle extremely high volumes of data, or dimensions of data". Data is defined above. In practice, in this sense "big data" is actually quite rare: "big" alone (just needing a distributed information system for some use case, but where data analysis is not the central goal of the system) or "data" alone (just running algorithms, even complex ones that also work on high-dimensional data, on moderate volumes of data) are much, much more frequent in practice.

	\item \textbf{Pipeline, data pipeline (A)}: a "data pipeline" is a term used to describe the set of structures and processes that allow humans to go from a raw measure of some domain to analyses of this domain. One should generally consider the following steps or aspects of a data pipeline.
	\begin{itemize}
		\item \textbf{harvesting / ingestion / collection / extraction}: this 1st step consists of obtaining raw data. This encompasses multiple aspects. Firstly, the choice of what's interesting and useable, metric-wise: this is mostly an exercise in modeling and understanding what is necessary to feasibly answer domain-specific questions. Secondly, the building of some tool that's capable of data extraction (this includes everything from survey forms, to mechanical thermometers). Finally, the agglomeration of these harvested metrics into some entity in the system, so that it maybe retrieved or transmitted later for further processing.

		\item \textbf{wrangling / preprocessing / transformation}: this 2nd step consists of preparing the raw data so that it can be used properly. A mathematical-programmatic model is chosen to represent the data. Raw data often has issues (inconsistent formatting, outlying errors due to failures of measurement, improper modeling by someone not conscious of the requirements of the rest of the pipeline or analytics) and these issues need to be resolved before that data can be used. Once the preprocessing is done, the data generally goes through a validation phase.

		\item \textbf{systems engineering / deploying / linking / networking / servicing}: this 3rd step consists of ensuring that setting up and managing access to the system, both internally and externally, is properly handled. This includes multiples aspects, such as:
		\begin{itemize}
			\item handling updates consistently (both to the data and the system itself);
			\item referencing content (such as setting up registry tables or distributed hashmaps);
			\item selecting the right hardware for the right role (storage, compute, caching, load balancing, logging, archiving, testing, monitoring, etc);
			\item network engineering (setting up, then optimizing, the interactions between machines);
			\item making sure only the appropriate parts of the system are accessible to the right actors (security analysis, system administration, pentesting);
			\item providing structured ways of speaking to the system for it to act on our behalf (such as APIs or query languages);
			\item making sure that the system can evolve (scalability, content distribution, code quality audits);
			\item monitoring, maintaining, and improving the overall health, performance and functionality of the system (end-to-end testing, logging, tracing, automatic fault recovery, audits, making backups, archiving, disaster recovery planning, vacuuming...).
		\end{itemize}

		\item \textbf{storing / loading}: this 4th step consists of putting the well-formatted, well-modeled, cleaned data into the appropriate databases and computers. There are various tradeoffs, mostly engineering-wise, that should be considered for this step. One can understand this step as "translating" an abstract mathematical model (such as a relational table, initially represented as some spreadsheets) into a specific technological architecture and implementation (such as an SQL database based on PostgreSQL). This step should above all consider how best to distribute the volume of data so that it can be accessed efficiently.

		\item \textbf{consumption / exploration / analysis}: this 5th step consists in actually using the data to produce useful (non-raw) "information": analyses. Generally one first starts with an exploration of data, trying to understand its various attributes, and its underlying geometry, using various visualization tools (such as software libraries like Matplotlib, or various algorithms like bar charts or more advanced ones, like doing a dimensionality reduction (e.g., PCA) first). Then, analyses are run. For example, a data scientist than elaborates hypotheses over the data, meant to test \textit{a priori} domain questions, or answer questions / remarks that arose from the process of exploration. In other cases, an ML engineer can choose an appropriate neural network model and define its topology in order to train models over the dataset. Finally, isolated reports or dynamic software that helps communicate or make this data useful for human decision-making, or to help process automation.
	\end{itemize}

	Note, however, that these steps are not neatly separated, nor neatly sequential. The above is a neat abstraction that allows one to have a simple mental model of the various factors required for a data pipeline. Dividing "data pipelines" into the above does not mean that all systems neatly follow the above division; far from it. You may very well have some ingestion immediately happen into a pre-existing, already well-networked system. You may have a company where the wranglers are also the data architects and write a single piece of software that goes from dirty raw data to everything being neatly ingested into a distributed database. The networking step can be done before the loading step if we know in advance the amount of data to ingest is going to be massive and are building a system from scratch; but it can also be done after, if we're scaling an existing small system into a bigger system. If the data is stored improperly, because the model was poorly thought-out, some wrangling is generally necessary before analysis. The list of exceptions and caveats goes on and on; however, given the overall complexity of modern systems, the above is still an excellent mental model to keep in mind, if one wants to have a simple set of questions to ask themselves when trying to analyze or design a system.

	Also note that there are peripheral steps that don't necessarily integrate into the pipeline \textit{per se}, such as running a prior analysis of the domain and prototyping models and systems on paper before even building anything; or the study of the pipeline itself in order to improve it over time; or documenting the pipeline for new workers.

\end{itemize}



\subsubsection*{Data generalities: professions}

This is a (non-exhaustive) list of professions, designed to give you an idea of the various roles that are available when dealing with data.

\begin{itemize}

	\item \textbf{Data wrangler (A)}: individual knowing how to create, synthesize, prepare, normalize, and/or homogenize data so that it is coherent and can be processed by algorithms.

	\item \textbf{Data engineer (A)}: individual knowing how to store, distribute, and route data so that it can be processed by algorithms on specific computing machines. They implement pipelines, and ensure data pipelines are efficient, scalable, and reliable.

	\item \textbf{Data analyst/scientist (A)}: individual who knows how to analyze input data, formalize a problem and elaborate hypotheses, choose an algorithmic protocol for data processing, provide visualization tools, and finally evaluate the quality of the predictive model generated by the algorithm. Their work is then used to aid strategic decision-making.

	\item \textbf{Data architect (A)}: A catch-all term for data engineers or mathematicians who are tasked with designing data pipelines. It is generally a managerial, or at least, a high-responsibility position. One can generally distinguish between \textbf{systems architects} (who design overall information systems, generally more on the engineering side; themselves divided into network architects, computer architects, web architects, etc) and \textbf{database architects} (who design how best to mathematically model the data from a given real-world domain in order to answer its questiosn, and help define how this data should be stored and serviced). Obviously, these profiles still do have a lot of overlap in their skills and responsibilities, hence the catch-all term.

	\item \textbf{Machine learning researcher (A)}: individual who uses mathematical, data science, and engineering principles to invent new machine learning algorithms, or improve existing ones (performance improvement, broadening of the application domain, etc).

	\item \textbf{Machine learning engineer (A)}: individual who uses data science and machine learning principles to implement existing ML algorithms to concrete business cases, e.g., automatic optimal logistics management. They work on selecting algorithms, preprocessing data, tuning model parameters, and evaluating model quality.

	\item \textbf{Database administrator (A)}: individual esponsible for managing and maintaining databases, including ensuring data integrity, security, backup, and optimization for efficient data retrieval.

	\item \textbf{AI ethics officer (A)}: invidual dedicated to ensuring ethical and responsible development and deployment of artificial intelligence and machine learning systems.

	\item \textbf{Data governance manager (A)}: individual responsible for establishing and implementing data governance policies and procedures to ensure data quality, security, compliance, and ethical use.

	\item \textbf{Data privacy officer (A)}: individual who oversees data protection measures, ensuring compliance with data privacy regulations and policies to safeguard sensitive information.

	\item \textbf{Chief data officer, CDO (A)}: high-level executive responsible for driving data strategy within an organization, overseeing data management, analytics, and aligning data initiatives with business goals.

	\item \textbf{Chief technological officer, CTO (A)}: high-level executive responsible for information systems strategy within an organization, overseeing hardware and software decision-making, cost (money and time) vs quality tradeoffs, and communicating technological stakes with the more "business" side of the company. To caricature, he CTO is the "highest technical position" in a company.

	\item \textbf{Vice-president of engineering, VPoE (A)}: high-level executives responsible for developer and engineer management, overseeing hiring decisions of technical profiles, promotions, team-management, and company culture for technical profiles. To caricature, the VPoE is the highest "tech HR" role.

\end{itemize}




\subsection*{Computer Science}

\subsubsection*{Computer Science: Programming}

This section describes terms that are fundamental to understand how one can use a computer to automate processes and ideas.

\begin{itemize}

	\item \textbf{Program (A)}: a program is a series of mathematical, software or electronics instructions or expressions; written either in a given programming language, or in machine code; stored as some file or set of files; and that can be executed as a common unit by a computer.

	\item \textbf{Algorithm (A)}: mathematical process capable of transforming inputs (data that is provided) into outputs (data that is generated), according to a determined and structured protocol.

	\item \textbf{Heuristic (BMELT)}: a technique of algorithmic design that allows one to trade precision or correctness of the solution against a faster calculation time. These are often used for very complex problems. Heuristic algorithms are algorithms that provide a "good guess" to a complex problem.

	\item \textbf{Programming language (A)}: a programming language is a way of writing text, usually a hybrid between English and mathematical language, to translate mathematical/algorithmic ideas into a form that an electronic computer can understand and execute.

	\item \textbf{Machine language (BMELT)}: a language based on the binary alphabet (containing series of just 2 symbols, '0' and '1'; or 'the current is blocked' and 'the current is flowing'), allowing a computer to process information automatically.

	\item \textbf{Type (AMELTOG)}: the category to which a given "real-world concept" belongs in a program. It is one of the most important concepts in computer science, both in practice and in theory. Mathematically, a type corresponds to the mathematical space to which a computer quantity belongs. We can distinguish, for example, the type "Integer" (signed integers), the type "Float" (floating point number, serving as an approximation of real numbers), the type "String" (textual character strings), "Bool" (true or false values), and their composites. Things like a "Player", or a "Color", in a video game are also examples of more complex types. Note that in what follows, the word "type" should be understood to have this specific definition.

	\item \textbf{Integer / Signed integer / Unsigned integer (A)}: an integer, or whole number, is a number that is used for counting. Unsigned integers, also called natural numbers, are the positive whole numbers (including zero). Signed integers, also called simply integers, are the positive and negative whole numbers (including zero). They are universal, and fundamental, to all programming languages.

	\item \textbf{Float / Floating point number (A)}: a floating point number is a number inside a computer meant to approximate real numbers (fractions, or general numbers with infinite fractional expansion). Floating point numbers rely on a special encoding that resembles scientific number notation, but instead based on powers of 2 rather than powers of 10. They are universal, and fundamental, to all programming languages.

	\item \textbf{Boolean / Proposition (A)}: a proposition, or boolean, is a value which can be either true or false. They are universal, and fundamental, to all programming languages, as well as philosophy.

	\item \textbf{String (A)}: a string is a sequence of characters (alphanumeric, punctuation, spaces, emojis, etc). It is a value which can be used to represent human language inside a computer. They are universal, and fundamental, to all programming languages.

	\item \textbf{Code (A)}: code is text written according to the rules of a programming language.

	\item \textbf{Syntax (A)}: the \textit{grammatical structure} of a language. For example, English and French are often simply described as languages where the order of words is "Subject-Verb-Object" (with some eventual Complement, indicating things like location or temporality, that can float around). Another example, the syntax of a programming language can restrict the programmer to declare the type before a variable (e.g., \texttt{Type\_A variable\_a}) or after the variable (e.g., \texttt{variable\_a : Type\_A}). An instruction ("sentence" of a programming language) that does not respect the syntax of its language will not be executable by the machine, because it is not translatable to machine language.

	\item \textbf{Semantics (A)}: the \textit{meaning} taken by an instruction/phrase in a language. An example of a semantic error would be \texttt{Integer my\_variable = "bobo"}: "bobo" is not an integer, so the statement, while syntactically correct (in a language like C), is semantically incorrect. Here's a famous example from "real-world" language by Noam Chomsky: "Colorless green ideas sleep furiously". This is not a grammatical error, it's a valid sentence, syntax-wise; however, there's clearly an error with the semantics: this specific association of words is meaningless.

	\item \textbf{Predicate (A)}: a predicate is a function that returns a boolean. These are very important in computer science, logic, and philosophy. And example would be "X exists", with X as input. "Water exists" would return true, but "Santa Claus exists" would return false.

	\item \textbf{Variable (computer science) (AMELT)}: in computer science, a variable is a "word" declared through writing, used to store a mathematical value in memory, and then use this mathematical value conceptually. For example, (\texttt{Float speed = 5.75} would declare a variable for "speed". In the rest of the code, the programmer would then use "speed" in the code (repeatedly and in the adapted way). This makes code much clearer to read than a "5.75" that hangs around, and is meaningless with a deep understanding of the context.

	\item \textbf{Function (computer science) (A)}: in computer science, function is a series of instructions callable from other places in the code, which can take zero, one, or more inputs and return zero, one, or more outputs. Functions can also affect the state (memory) of a program or computer (side effects). This is for example the case for functions that display information on a screen: they affect the electronics of the computer. A "pure" function is one which does not have side effect: one which is a purely mathematical calculation that can be figured out with pen and paper, given some inputs.

	\item \textbf{Condition (AMELT)}: A condition is a statement that translates a logical calculation (Boolean, a question with a true/false answer) into a conditional redirection (a branching) of the code. Keywords: \texttt{if, elif, else, and, or, not, then}.

	\item \textbf{Loop (AMELT)}: A loop is a series of instructions that can be repeated, with minimal configurable changes, as long as a given condition remains true.

	\item \textbf{Turing-completeness (BMELTG)}: roughly speaking, a programming language is said to be "Turing-complete" if it is "as expressive as possible". That means that it is able to execute computations on all types, store information in its memory, execute its computations contextually/conditionally, and redirect its reading head. Turing-completeness is a fundamental notion of theoretical computer science, and for reasons that are long and complex to explain, it is in a way the ultimate "speed limit" to computation. A Turing-complete language is able to execute any (reasonable) function. Almost all programming languages have a Turing-complete level of expressivity.

	\item \textbf{Argument (AMELT)}: an argument is a synonym for a function input.

	\item \textbf{Return (AMELT)}: a return (value) is a synonym for a function output.

	\item \textbf{Signature (AMELT)}: the signature of a function is the declaration of the name of the function, and the types and names of its inputs and outputs. The function's type is defined as "types of inputs -> types of outputs", and is a direct consequence of these.

	\item \textbf{Interpreted language, compiled language (AMELT)}: a language is said to be "interpreted" if a software called "the interpreter" must be launched to read the code line by line, as the code runs, so that it executes. A language is said to be "compiled" if a software called the "compiler" must read the whole code and produce an executable ahead of time, before the program can be executed. Python is an example of an interpreted language. C is a compiled language.

	\item \textbf{Static typing, dynamic typing (AMELT)}: A "typing" is how the type system for a programming language was designed. It is said to be static if it is necessary at the level of the programming language's syntax, or if it can be inferred before the code is run. A typing is said to be dynamic if the interpreter or software is only able to infer the type of a computer value from its context, at runtime (and thus the declaration of the type of a variable or the arguments of a function is not necessary). Python is a dynamically typed language. C is a statically typed language. Statically typed languages are generally more restrictive in their syntax, but also generally more rigorous and reliable.

	\item \textbf{Paradigm (of a programming language) (BMELTO)}: way to design a programming language. The 3 most frequently used paradigms are: imperative, object-oriented, and functional. You may also see the term "procedural" used instead of "imperative". These paradigms are not necessarily mutually exclusive. Imperative languages are very close to how the machine works, giving orders as concrete instructions (typical example: C). Object-oriented languages structure their components in "classes", types that contain both data (i.e., state: nouns or adjectives) and functions (i.e., actions: verbs). Most modern languages are inspired by object-oriented design; Python included, among others. Functional languages are very close to mathematics and are inspired by the lambda calculus; they have the advantage of producing very solid code because they are close to a mathematical proof, when they are handled by the interpreter or the compiler. It's helpful to think of the history of programming, with one side (functional) starting from the works of mathematicians, and the other starting from the works of electronics engineers (imperative). There are other paradigms (such as array-oriented), though their use is much less frequent.

	\item \textbf{Version control / Git / GitHub / GitLab / Bitbucket (AMELTO)}: Technology for managing project data and archiving code, used in software development. There are many historical ones, but today, git has become the \textit{de facto} norm. A git "repository" contains all the archives and changes in a project's data since its creation. This means that you can use it to find anything that was saved at any time in a project's history (up to voluntary deletion of that history). Platforms like GitHub, GitLab and BitBucket allow sharing of, and collaboration for, code projects online. Using git daily is a \textbf{\textit{fundamental}} practice when programming, \textit{and} when collaborating with programmers.

\end{itemize}



\subsubsection*{Computer Science: Hardware}

This section describes the important electronic components that allow an electronic computer to work.

\begin{itemize}

	\item \textbf{Computer (A)}: There are two major definitions of computer: one is a mathematical model for "how to automatically do math" (see Turing-completeness); another is a physical (generally electronic) machine, serving as a platform for general computation (see Von Neumann architecture). The more common definition is this second one.

	\item \textbf{Central Processing Unit, CPU (AEL, BMTG)}: the fundamental calculation unit of a computer.

	\item \textbf{Graphics Processing Unit, GPU (AEL, BMTG)}: a computing unit specialized in parallelized computing. Modern GPUs are efficient in performing tasks like simultaneous scalar products, making them suitable for tasks involving linear algebra.

	\item \textbf{Register (AE, BMLTG)}: a memory unit that contains data directly accessible to the CPU, and over which it can run its computations. It is the fastest and smallest kind of memory generally available on a computer.

	\item \textbf{Random Access Memory, RAM (AE, BMLTG)}: a memory unit that contains data and software currently running on the computer. It is cleared as soon as the computer turns off. It is generally produced in the form of flat stick of complex electronics. It is a medium sized, medium access speed memory component.

	\item \textbf{Read-Only Memory, ROM (AE, BMLTG)}: a memory unit that contains inactive but saved data and software of the machine. It is generally in the form of a rotating hard disk or a flash memory drive. It is a big sized but comparatively slow accessed memory component.

	\item \textbf{Motherboard (AE, BMLTG)}: The control unit that contains most of the software necessary for the computer to start. It serves as an interface to various hardware components.

	\item \textbf{Power supply unit, PSU (AE, BMLTG)}: An electronic unit that provides and adapts power from an electric sector outlet to the computer, ensuring its proper operation.

	\item \textbf{Port (computer electronics) (AE, BMLTG)}: A female-type (hole-like) plug on a computer that the motherboard can use to interface with other electronic components, such as external memory drives, sound systems, or network cables.

	\item \textbf{Card (sound card, network card, etc) (AE, BMLTG)}: A specific electronic component meant to handle some specific function which generally is not simple computation or handling of memory. This includes managing software and devices used to connect to computer networks, or producing sounds from binary data and programs. For "graphics card", see GPU.

\end{itemize}



\subsubsection*{Computer Science: Data Types}

This section describes various data types that are fundamental to algorithmics in general, and data engineering/science in particular.

\begin{itemize}

	\item \textbf{Array / List (AMELTG)}: an array, or list, is a series of values, usually of the same type, stored (usually) contiguously in memory, and accessible by index. In the vast majority of languages, the indexing of an array starts at 0. For example, if you have the following array of integers \texttt{Array<Integer> my\_array = [1, 4, 6, 10, 0]}, then \texttt{array[2]} returns 6. In languages where a distinction is made between arrays and lists, arrays are contiguous in memory, and lists are implemented as \textbf{linked lists}, which consists of a series nodes at various places in memory, each node containing both a value, and a "pointer", indicating where the next vaue can be found in memory. "Doubly" linked-list have each "next" node in the list also point to the previous from which it was referred. "Cyclic" lists have the last node point to the first.

	\item \textbf{Tensor (computer science) (AML, BETG)}: a tensor is an array of arrays of arrays of arrays of arrays... of elements usually of the same type. The "rank" of a tensor is its nesting level. A simple value is a tensor of rank 0. A simple array is a tensor of rank 1. An array of arrays is a rank 2 tensor. An array of arrays of arrays is a rank 3 tensor. Etc. An Excel spreadsheet with only number is an example of a rank 2 tensor.

	\item \textbf{Graph (A)}: a graph is a set of points (usually of the same type, called nodes), which are linked together 2-by-2 (by connections called edges). A very rudimentary social network, for example, can model its users as "one node per person" and "one edge between two people if they are friends". Graphs can be \textbf{oriented}, if the links are directional (such as the "parenthood" link between a parent and their child), or \textbf{non-oriented}, if the links always go both ways (such as "being related by blood"). If a graph has numerical values (on its nodes or edges) it said to be "\textbf{weighted}". If it has qualifiers (on its nodes or edges) it is said to be "\textbf{labeled}". The \textbf{distance} between two nodes of a graph is the length (in amount of edges, or sum of edge weights) of the smallest path between these two nodes. The "\textbf{diameter}" of a graph is the maximal distance between any pair of nodes in the graph. An edge from a node to itself is called a "\textbf{loop}".

	\item \textbf{Tree (AMELTG)}: a tree is a graph without cycles. One of the nodes of a tree can singled out to be the source of some operation on the tree, or some structure, in which case it is called the tree's \textbf{root}, in this case, we tend to speak of "parent" nodes and "child" nodes, where the root is the universal common ancestor (even if the tree is non-oriented). A node that has no children is called a \textbf{leaf}. The \textbf{depth} of a tree is the distance between the root and its furthest leaf. There are special kinds of trees, such as binary trees (where every node has at most 2 children), binary search trees (BST, weighted binary trees where each left child contains only values inferior to the parent, and each right child only values that are superior), B-trees (which generalize BSTs to $n$ possible children), red-black trees (a type of BST where one can ensure a certain amount of balance between branches, i.e, minimizing tree depth). 

	\item \textbf{Stack / Last-in first-out, LIFO (A)}: a stack is a fundamental data structure, consisting of blocks of the same type, and just 3 operations: push (adding an element to the top of the stack), view (looking at the value of the element at the top of the stack), and pop (removing the element from the top of the stack). At its most primitive, mathematical level, a program's memory is generally modeled as a stack. This concept also has applications in logistics, when it comes to strategies for handling inventory.

	\item \textbf{Queue / First-in first-out, FIFO (A)}: a queue is a fundamental data structure, consisting of blocks of the same type, and just 3 operations: enqueue (adding an element to the back of the queue), view (looking at the value of the element at the front of the queue), and dequeue (removing the element from the front of the queue). This concept also has applications in logistics, when it comes to strategies for handling inventory.

	\item \textbf{Data point cloud / Data frame / Dataframe / Data point space / Data table / Relational table (A)}: a data frame is two-dimensional table, with "individuals" (aka "entities") as rows, and "attributes" as columns. When its data is visualized geometrically, it corresponds to a point cloud in a mathematical coordinate frame, where each column of the table defines a "dimension", i.e. an axis of the coordinate frame. (Note that for non-numeric columns, these can generally require 2 axes, one for the various labels/categories, and 1 for some metric, generally a count of values.) Each entity corresponds precisely to a single point in this frame. Most data tables have too many dimensions for the human brain to visualize (since we are limited to 3 spatial dimensions, 1 temporal dimension, and possibly 1 or 2 color gradients, for a total of 6, which is in practice very rarely attainable in an understandable way). Mathematically, if one considers each attribute as a given set (or type), then a dataframe can be understood as a subset of the repeated cartesian product of the attributes.

	\item \textbf{Simplex (BML, CET)}: (pl. simplices) an $n$-simplex is the $n$-dimensional equivalent of the $1$-dimensional line, $2$-dimensional triangle, $3$-dimensional tetrahedron, etc.

	\item \textbf{Simplicial complex (BML, CET)}: a simplicial complex is a collection of connected simplices. Triangle meshes from computer graphics (video games and CGI) are a kind of simplicial complex.

	\item \textbf{Hypergraph (BML, CET)}: a hypergraph is a graph where edges can be $n$-ary, rather than just $2$-ary. More generally, a hypergraph is any subset of the power set of a set of nodes. (Non-oriented) graphs and simplicial complexes are specific kinds of hypergraphs.

\end{itemize}



\subsubsection*{Computer Science: Computer engineering}

This section has some vocabulary which useful for computer engineering, on the software side. This can also be understood as a section on low-level software engineering. Note that there is also another section on systems engineering, and yet another network engineering, specifically.

\begin{itemize}

	\item \textbf{Operating System, OS (A)}: an operating system is a piece of software that allows the management and operation of other software. It is generally the first piece of software that you want to run when turning on a computer. Examples of such architectures include Mac OS, Windows, Linux-Debian, Linux-RedHat, iOS, Android, Raspberry Pi, Microsoft Azure, and more.

	\item \textbf{Bootloader (BE, CG)}: For most modern computer architectures, since modern OSes are quite complex to launch, a smaller operating system, called a bootloader, is used to launch the main operating system (in fact, this can be a multi-stage process where a bootloader loads another, more complex bootloader). Examples of first-stage boot-loaders include BIOS and UEFI. Examples of second stage bootloaders include GNU GRUB and rEFInd. In more complex (but important) scenarios, the bootloader and OS's data can be received from some other computer in the network at startup.

	\item \textbf{Cross-platform (AE, BMLTO)}: Refers to software or code that can run on various operating systems.

	\item \textbf{Process (computer engineering) (A)}: Refers to an isolated, identified, and managed piece of software that is running on an operating system. Whereas a program is the "specification" of instructions that a computer should run, a process is the form that a program when it is "run" (executed) by a computer. A program can instantiate of multiple processes (a process called forking).

	\item \textbf{Kernel (computer engineering) (AE, BG)}: the kernel of an operating system is the fundamental "manager" program of the computer, which is tasked to control how other programs run. Its tasks include:
	\begin{itemize}
		\item creating, registering, monitoring, managing and killing processes;
		\item scheduling and executing tasks required by processes;
		\item allocating (or refusing to allocate) memory and compute ressources to processes when they ask for them;
		\item handling user management, and the subsequent permissions that various processes are granted;
		\item handling fundamental errors (interrupts);
		\item handling the filesystem (this includes files, but also peripheral devices, network connections, etc.);
		\item etc.
	\end{itemize}

	\item \textbf{Bit (A)}: a bit is the fundamental unit of information. It corresponds to a choice 2 values: 0 or 1 (equivalently, "true or false", or yet again, "passing current or blocked current"). A sequence of bits (such as $1000111011$) is called a "word in binary language".

	\item \textbf{Byte (A)}: a byte is an ordered sequence of 8 bits, corresponding to a choice between $2^8 = 256$ values. It is the fundamental unit that modern computers use to divide information into organized chunks. 

	\item \textbf{Address / Memory address (AE, BMLT)}: a memory address is a specific location on a computer, described as a numeric integer value, which contains some data, usually the size of a byte.

	\item \textbf{Pointer (AE, BMLT)}: a pointer is value of a specific type in imperative programming, one which expresses a memory address in the computer as an integer value. Pointers are a fundamental construct in imperative programming.

	\item \textbf{Memory buffer (AE, BMLT)}: a memory buffer is a fixed-sized ($n$ bytes) piece of computer memory, allocated for some process by the kernel, in which binary data can reside. The address of a memory buffer is generally taken to be that of the first element in the buffer.

	\item \textbf{Cache / Memory cache (BE, CMLT)}: a memory cache is a memory device (or area in a memory device) reserved to store values that were either recently used, frequently used, or both (depending on the strategy). Caches are one of the major ways to engineer speedups in memory retrieval, but they do have some cost. Some types of caches exist on a single machine, while others can be used to speed up data retrieval from a network of machines.

	\item \textbf{Thread (BE, CMLT)}: a thread is a specific resource that can be allocated to allow multiple parts of a program (or the same part, but multiple times) to run simultaneously. Threads are the fundamental concept of concurrent and parallel programming. Ending a thread (and eventually returning to sequential programming) is called "joining" the thread. All threads generated by a process are dependent said "parent" process (if the process is killed by the OS, .

	\item \textbf{Readhead (BMELT)}: a readhead is a location at which the code is currently being read. Sequential (synchronous and single-threaded) programs have a single readhead at any time. Concurrent (multi-threaded and/or asynchronous) programs typically have multiple readheads.

	\item \textbf{Task (BE, CMLT)}: a task is an ambiguous term describing a unit of work in a computation. This catch-all term is used when the actual implementation of a concurrent program is unknown and we need to think about it abstractly. To give an idea, a task might be a sequence of instructions that a thread repeats regularly, or the unit of work done by a single thread over its lifetime, or the unit of work done by a full process. We then use tasks to describe architectures where the question of "how do we execute multiple tasks ?" is key. In certain programming languages (like Ada or Erlang), "tasks" are a much more well-defined concept, used explicitly in the respective language's concurrency model.

	\item \textbf{Concurrency / Concurrent programming (BE, CMLT)}: Concurrent programming refers to the act of writing a program so that its process (or processes) can read various parts of the program (or the same part, multiple times) simultaneously. Concurrent programming has 2 main goals: efficiently switching between tasks to make the most of available resources (such as having a music player and a browser run simultaneously on an OS, or calculating various aspect of a video game's graphics at the same time), or making sure that processes that \textit{need} to run at the same time \textit{can} run at the same time (such as being able to interact with a program's window and having the program communicate with the network in the background). There are two main types of tools for concurrency: multithreading and asynchrony. Concurrent programs are tough to write, and can lead to very harmful, difficult to resolve bugs, namely deadlocks, and race conditions. All modern CPUs allow for concurrent programming.

	\item \textbf{Parallelism / Parallel programming (BMELT)}: parallelism refers to a specific kind of concurrency, when (roughly) the same task needs to be run on multiple threads, or processes, or machines, at the same time. The most famous parallel compute device is the GPU. We can distinguish between shared-memory parallelism (on a single machine) and distributed parallelism (over multiple machines, and memory sharing is done by network communication).

	\item \textbf{Single instruction single data, SISD (BEL, CMT)}: refers to sequential programming. In a SISD architecture, there is only one thread at any time, and it reads the code sequentially.

	\item \textbf{Single instruction multiple data, SIMD (BEL, CMT)}: refers to parallel concurrency. In a SIMD architecture, multiple instances of data of the same type, but with differing values, are processed by multiple threads, executing the same program simultaneously, instruction-by-instruction (hence the "single instruction" principle). An example is as an array of 3D vectors, where each 3D vector in the array is to be processed by the same algorithm. The fundamental problem of SIMD programming are conditional branches ("if/else" statements) which may cause slowdowns, because these break the single-instruction principle. SIMD is the way that GPUs do their calculations.

	\item \textbf{Multiple instruction multiple data, MIMD (BEL, CMT)}: refers to general concurrency. Any instruction sequence can be applied concurrently to any data. MIMD is the way that modern (multi-core) CPUs do their computations. Any MIMD device can run some version of a SIMD program (to some extent), but they might not be as specialized and effective as a device designed for SIMD (such as a GPU).

	\item \textbf{Multithreading (BEL, CMT)}: multithreading a program is the act of having the program create multiple threads in order to run concurrently. If a program is a restaurant's kitchen, multithreading is like hiring multiple cooks, so that each can handle a given task. One can write a program that is both multithreaded and asynchronous, since these are 2 different concepts allowing concurrency.

	\item \textbf{Asynchrony (BE, CMLT)}: asynchrony is a way that a program can run concurrently on a single thread, by having multiple readheads, and switching between these. If a program is a restaurant's kitchen, asynchrony is like a single cook preparing a sauce, vegetables, meat, etc, simultaneously in their own respective pot, by doing each step that should be done as soon as it can be done, but never being idle, and moving from pot to pot regularly. For example, asynchrony can be implemented via things like an event loop (which manages the different readheads and their execution order on a single thread), coroutines (which allow functions to suspend themselves and let their parent start executing again), or message passing between readheads (where a readhead starts up only when it receives a specific message from another readhead). Typical constructs through which asynchrony can be used in programming languages include: callback functions (a function that is passed as an argument, and is to be executed once the fed function returns), (monadic) promises or futures (which are type wrappers (functors) which allow the programming language to know that their content is running asynchronously), or async/await keywords (used to specify whether a function is asynchronous, and whether a certain call of this function should be blocking). Asynchrony is a very effective mechanism to handle operations that require lots of IO (typically, handling multiple simultaneous network connections; or reading/writing from/to multiple files) while using only minimal resources (e.g., a single thread). One can write a program that is both multithreaded and asynchronous, since these are 2 different concepts allowing concurrency.

	\item \textbf{Mutex / Lock / Semaphore (BE, CMLT)}: a mutex (for "mutual exclusion") is a tool that multithreaded or asynchronous programs use to avoid race conditions and deadlocks. A mutex lock defines a section of the code that should only be accessed by a single readhead at any time, sequentially; a mutex unlock defines the moment where the code can be read concurrently again. There are multiple possible implementations for mutexes, but the following gives a good idea of the expected behavior. A mutex is basically a global counter that is incremented each time the "lock" operation is called, and decremented at each "unlock". When a readhead arrives at a mutex lock, if the counter is at 0, it increments it to 1, and keeps reading what follows. If another readhead arrives at the lock, it increments it to 2, and gets put into a queue. Once the first readhead reaches the unlock, the global counter is decremented to 1, and the first readhead in the queue can now get its turn to execute. Typically, mutexes are used when trying to get a resource from a resource pool that is shared between threads and/or asynchronous readheads (such as available connections for a server).

	\item \textbf{Deadlock (BE, CMLT)}: a deadlock happens when multiple threads or readheads are trying to access the same resource(s) and some readhead is refusing to release resources that are necessary for the program to continue. A typical example is the dining philosophers problem, which we will caricature and generalize here. We have two parties (readheads), A and B, that each needs 2 resources (R1 and R2) to do their respective task. Neither will release a resource if they have it, until their task is complete. The program is running concurrently, so both A and B can act simultaneously. A picks up R1. B picks up R2. Neither A nor B can now pick up both resources, and neither will release their own resource. The program is stuck, it has reached a deadlock.

	\item \textbf{Race condition (BE, CMLT)}: a race condition happens when a resource is accessed simultaneously by multiple readheads and this leads to logical inconsistencies (which can lead to crashes in the worst case). Say two parties (readheads), A and B, share a pair memory registers to run an addition. A writes 5 to the first register. B writes 1 to the first register. A writes 7 to the second register. A runs the addition and obtains 8, rather than the expected 12. Generally, race conditions are non-deterministic bugs, since the overall result depends on the order that concurrent readheads accessed the resource, which varies upon multiple executions of the same program. For example, here, if B had had the time to write 4 to the second register before A ran its addition, A would have obtained 5 instead.

	\item \textbf{Vectorization (BMELT)}: Vectorization is the expression of computations on arrays (\textit{a fortiori}, tensors) in such a way that the computations can take place simultaneously (to leverage parallelism). This works according to the SIMD (Single Instruction, Multiple Data) principle: you run exactly the same operations on lots of data of the same type, at the same time. It is this principle that allows GPUs to do graphical computations or computation for data or neural networks faster than CPUs.

	\item \textbf{Clock (BE, CMLT)}: a logical electronic circuit's (\textit{a fortiori}, a computer's) clock is a device that sends regular, evenly timed, electronic signals, like a beat or metronome. This allow operations to be synchronized, which is essential to avoid things like deadlocks and race conditions that could happen through the raw, physical irregularities of electronic components. To increase the manufacturer's default clock rate on a computer's CPU, in order to improve performance (at the potential cost of excess energy consumption, heat generation, and component failure or damage), is called "overclocking".

\end{itemize}



\subsubsection*{Computer Science: Network engineering}

This section has some vocabulary which useful for network engineering. Note that there is also another section on computer engineering, and yet another systems engineering specifically. 

\begin{itemize}

	\item \textbf{Server (A)}: a server is a piece of software that allows other computers to interact with it. You can think of a "server" as a program "at whose door you can knock".

	\item \textbf{Client (A)}: a client is a piece of software that can make requests to another computer. Web browsers are the typical "client" software that most people know about. You can think of a client as a program "that goes to knock on other people's door".

	\item \textbf{Peer (AG)}: a peer is a piece of software that is both a client and a server.

	\item \textbf{ISO model (of network layers) (AE, CG)}: a description of the various layers required for modern computer networks to functions, down from electronics, up to applicative logic (TODO: go into more detail).

	\item \textbf{Data packet (AE, BG)}: a memory buffer containing specific data, which is given a "header" (small chunk of data to provide addressing, verification, or other information) in order to be replicated from machine to machine and reach a specific destination within a computer network.

	\item \textbf{Internet (A)}: the internet is a extremely large network of computers, all put into a common basis for communication via some software protocols (most important of which are IP, TCP, and UDP).

	\item \textbf{Internet Protocol, IP (AG)}: the internet protocol is a collection of software and protocols allowing the transmission between two machines that are not directly linked, but need to go through an intermediary network to talk to each other. It is the fundamental building block of the modern internet.

	\item \textbf{Transmission Control Protocol, TCP (AG)}: a network protocol allowing the streamed transmission of data packets from computer to computer. Unlike UDP, TCP checks if the packet sent was well-received (complete, uncorrupted) via supplementary communication from the receiver to the sender. Most network communications (except video streaming) are sent via TCP, and brought to destination via IP, therefore, we often refer to their combination as \textbf{TCP/IP}, which is the basis for the modern internet.

	\item \textbf{User Datagram Protocol, UDP (AG)}: a network protocol allowing the streamed transmission of data packets from computer to computer. Unlike TCP, UDP does not check if the package was well received. This allows software using UDP to keep sending data, regardless of its arrival status, thereby greatly improving communication performance and scalability. It is in particular the protocol used for video streaming.

	\item \textbf{HyperText Transfer Protocol, HTTP (A)}: a protocol used as a way of requesting or sending data so that it can be used by software called a "browser" to visualize or run content, (almost always) by receiving it from some distant computer (a server). HTTP runs over TCP/IP, providing it a form of semantics.

	\item \textbf{Socket (AE, CG)}: a socket is a software-level numerical identifier for a connection within a program. It allows programmers to establish distinct connection to other machine via IP within their software. Sockets work by requesting specific resources from the operating system.

	\item \textbf{Port (AE, BG)}: a port is an OS-level numerical identifier that allows one to distinguish between different types of protocols when communicating. The only ports that are (legally) open by default on every OS are port 80, for HTTP, and 443, for HTTPS. Of note is port 22, for the SSH protocol.

	\item \textbf{Firewall (AE, BG)}: a firewall is a specific piece of software that manages which ports are open and which are closed (both for outgoing and incoming connections), in order to protect a given OS from foreign intrusion.

	\item \textbf{Cryptography (A)}: from the greek words for "hidden" and "writing/drawing", cryptography is the science of being able to communicate messages in a way that their information can only be read by the intended receiver. Cryptography dates back to early Antiquity and has a truly fascinating history, touching frequently on political, military, and scientific history. 

	\item \textbf{Public-private-key cryptography, PPK cryptography / Asymmetric cryptography (A)}: describes the fundamental way that computers communicate cryptographically. The idea is to use the solution of a mathematical problem as a key to make an encrypted (illegible) message legible. If you can solve the problem, you can use its solution to get access to the message, but the problem would take even the most powerful computers unreasonable amounts of time to solve. On the other hand, verifying that the solution is correct is very easy and quick to do (just like solving a Sudoku vs verifying that a filled-out sudoku is valid). Admittedly the idea of a "public key" is a bit of a misnomer. Think more of a "public key" as an "open treasure chest that locks down as soon as you close it, that you can duplicate an infinite amount of times". It is what allows one to create a "complex mathematical problem" from their message, in a way that is hard to rewind, unless you know the solution for all messages. Party A (referred to as Alice) wants to send a secure message to party B (Bob). Alice asks Bob for his public key (treasure chest). Alice writes her message and encrypts it with Bob's public key (she puts it in her copy of Bob's treasure chest). Now, even Alice can't open the treasure chest ! She sends the encrypted message (treasure containing the message) back to Bob. Bob has the "private key", the solution to the problem (the key to the treasure chest that must never be shared). He can use it to decrypt the message (open the chest) and read Alice's message.

	\item \textbf{Peer-to-peer network (A)}: a peer-to-peer network is a network in which every node is a peer. Peer-to-peer networks are powerful because they are highly distributed, making for efficient transmission of data, or a good source of decentralized (but federated) compute.

	\item \textbf{Secure shell, SSH (A)}: a communication protocol used to securely access a user's terminal on a distant machine. Uses port 22 by default, but a firewall might block port 22 and reserve another (hidden) port for SSH instead, in order to make a machine more secure by obfuscating the entry port.

\end{itemize}



\subsubsection*{Computer Science: Systems engineering}

This section has some vocabulary which useful for systems engineering. Note that there is also another section on computer engineering, and yet another network engineering specifically. 

\begin{itemize}

	\item \textbf{Relational database / SQL database (AMELTO)}: a database is a set of (usually massive) data tables, possibly logically linked together by 2-column tables called "junction tables". These have been the most important data structure for a long time. Relational databases have an advanced mathematical formalism, called "\textbf{relational algebra}", developed by a certain Edgar Codd. There are programming language specific to interactions with databases, including the SQL family of languages. Databases are so frequently relational that anything else is generally referred to under the umbrella term "NoSQL".

	\item \textbf{Normalization / Relational database normalization (AE, BMLTO)}: the relational algebra specifies what are called "\textbf{normal forms}" for a relational table. These allow the prevention of a certain amount of database problems generally referred to as "\textbf{anomalies}".

	Here are the standard anomalies which one needs to look out for. Our example uses a fictitious, non-normalized database, where data about a player in an MMORPG, and their inventory, are stored in the same table, with roughly one row per inventory item.  
	\begin{itemize}
		\item deletion anomaly: deleting a row (such as a player consuming their last item) deletes extra unrelated info (such as the last reference to the player itself);
		\item update anomaly: updating a row (out of two that should be updated, such as the price of only one of two items) leaves an incoherence (typical when there is duplicate info);
		\item insertion anomaly: adding known information (such as "a new player should begin the game with a normal status") requires adding a row, which can't be added because other info is lacking (they start with an empty inventory, so we can't neatly add the row).
	\end{itemize}

	The following describes how the requirements a table must meet to be in a given rank of normal form.
	\begin{itemize}
  		\item 1NF requirements: presence of a primary key (a column, or group of column, from which one can uniquely distinguish each entity), unordered rows (the order of rows should not provide any semantic value; the list of entities is actually not a list, but a mathematical set), simply-typed columns (the content of column is consistently typed, no mix of string and int, etc.), no repeating groups (aka atomization: no cell should contain a variable amount of information; no list in a cell).
  		\item 2NF requirements: the table is 1NF, and when a primary key in the table is composed of 2 fields, each non-key attribute (column) must depend on (=can only be infered through) the entire primary key.
  		\item 3NF requirements: the table is 2NF, and no attribute depends on another transitively. 
  		\item BCNF, 3.5NF (Boyce-Codd NF): "every attribute in all tables should depend on their whole primary key, and only on their primary key".
  		\item 4NF requirements: the table is 3NF, and the only maltivalued dependencies allowed are multivalued dependencies on the key.
  		\item 5NF requirements: the table is 4NF, and cannot be described as the result of a junction of other tables.
  	\end{itemize}

  	There are some case where one could want to denormalize a table, for example for performance reasons in a database which is read-only.

	\item \textbf{Index / (Relational database) index (AMELT)}: an index is a data structure, generally some form of B-tree (possibly a BST or red-black tree), where the leaves of the tree are additionally structured as a doubly-linked list. These allow the retrieval of information in logarithmic time complexity.

	\item \textbf{NoSQL database (AMELTO)}: a NoSQL database is an umbrella-term for any database that isn't SQL. Some of them are even SQL-like, but with extra structure. Here is a list of frequently seen NoSQL database architectures.
	\begin{itemize}
		\item graph database: useful to model "ecosystems" or networks, any type of domain where there are analogous element between which there are many relations. Generally comes with a query language, like Oracle's property graph query language (PGQL). A famous example is neo4j.

		\item key-value store: a way of organizing data as key-value pairs (typically, hashmaps). Keys are generally strings, values are generally primary types, arrays, or dictionaries. There is no specific interaction language like SQL for them; so managing their content is generally handled in a custom manner at the application level.

		\item blob (binary large object) store: used to stores lots of binary data files (videos, music, large text, exe, etc). Famous examples include Google cloud storage, Simple Storage Service (Amazon S3), or Azure blob store.

		\item time series database: stores a bunch of data which consists of metrics that happen at regular, sequential timed events. Famous examples include influxDB and prometheus.

		\item document database: stores data as documents containing JSON or BSON data (structured, but with variable schemas) that are spread across machines. JSON and BSON can be described as "key-value trees" or "nested key-value pairs". Collections of documents can be indexed, nested or linked. A famous example is MongoDB.

		\item full-text database: works like the index at the end of a book (finds a term via a hashmap of terms-to-locations), and is used for building performant search engines. Examples: solar, elasticsearch, lucene, algolia, meilisearch.

		\item columnar TODO

		\item wide-column / column-family store: like a key-value store, but with a bit more depth. The values can generally be understood as denormalized relational tables. These are good for high-write low-read (eg, lots of time series data, or event logs), and specific filtered per-column reads (e.g., Apache Cassandra).

		\item multimodel: is said of systems that will try to cleverly design what's required as a data model for you, based on your specification (such as fauna with GraphQL).
	\end{itemize}

	\item \textbf{Structured Query Language, SQL (AMELTO)}: SQL is a programming language which is not general purpose, but instead specifically tailored to interacting with a relational database. There are multiple variations on the SQL language for various other database technologies (such as MySQL or PostgreSQL).

	The various types of SQL queries are often classified under the acronym "\textbf{CRUD}".
	\begin{itemize}
		\item Create (CREATE, INSERT): allows things like creating new tables, indices, or adding rows to existing tables;
		\item Read (SELECT): allows to specifically select, and/or combine, and/or format, and/or filter, and/or sort, rows from one or more tables.
		\item Update (UPDATE): allows to find and edit one or multiple rows' attribute(s).
		\item Delete (DROP, DELETE): delete a row from a table, or delete a full table.
	\end{itemize}
	However, some SQL operations are not of this kind, but still useful, such as the EXPLAIN/ANALYZE family of commands.

	When it comes to custom queries, these tend to mostly be Read type queries. For this reason, it is important to understand the general order of execution of the various components of an SQL read query. This is necessary, since you can get to the same result in various ways, and some ways are much more costly than others.
	\begin{itemize}
		\item SELECT: choose columns, then (optional) apply a map/function to its values, and put function's result into a kind of "variable", which acts as a new temporary column;
		\item FROM: choose the source table;
		\item JOIN: choose second table source and define the kind of join (from least costly to most costly: INNER, LEFT or RIGHT, CROSS);
		\item ON: express which columns to match in FROM / JOIN;
		\item WHERE: predicate to filter based on column value for the chosen column(s);
		\item GROUP BY: fold on specific column;
		\item HAVING: also a filter, but unlike WHERE, works for \textit{grouped} records;
		\item ORDER BY: sort by a specific column;
		\item LIMIT / TOP: cuts to keep only the first $n$ results.
	\end{itemize}

	\item \textbf{Transaction / Database transaction (AE, BMLTO)}: a transaction is a logical unit of work when writing to, or reading from, a database. A transaction can be composed of multiple sub-operations.

	\item \textbf{ACID (BMELTO)}: this is an acronym which describes the key properties that must be respected for a database transaction to work properly and be properly designed. Most modern database technologies are said to be ACID-compliant because they adhere to these principles.
	\begin{itemize}
		\item \textbf{atomicity}: either the whole transaction is executed, or it fails fully (e.g.: no adding money to someone without removing money from someone else). If the operation succeeds, we commit the result (put it into effect). If some operation fails, we rollback to the state before the transaction started.
		\item \textbf{consistency}: transactions must respect logical constraints (specified by the domain) on the data (e.g.: no one can spend more money than they are stated to own in the database).
		\item \textbf{isolation}: in effect, transactions are applied sequentially (e.g.: if two people try to concurrently retrieve money from a shared account, this principle prevents them from withdrawing a sum of money greater than the account owns overall).
		\item \textbf{durability}: valid transactions are perennial and survive a system failure. If a system or machine fails mid-transaction, the correct state can be recovered. This is ensured by some things like \textit{write-ahead logging}, where before any transaction or commit, we write to a file the transaction that is to be committed. If the transaction is committed successfully, that log file is cleared of the now validated transaction.
	\end{itemize}

	\item \textbf{Distribution / Distributed computation / Distributed system (AMELTO)}: refers to the act of having a program run as processes over multiple machines in a network, such a network being called a "distributed system".

	\item \textbf{CAP theorem (BE, CMLT)}: the CAP theorem (standing for Consistency/Availability/Partition tolerance theorem) is a theorem which states that out of keeping data over a system globally consistent, available, or globally connected (without a partition of the overall network graph into 2 pieces or more), only two of those three can be ensured at any time. In practice, since faults are unavoidable, this boils down, when a partition happens, to either choosing consistency (preventing transactions because they can't be ensured to be consistent, since part of the network is unreachable) or availability (allowing transactions to happen, knowing that they'll need put the system in an inconsistent state which will need to be resolved once the network is fixed and the partition disappear).

	\item \textbf{Sharding / Vertical sharding / Horizontal sharding (BE, CMLT)}: sharding refers to the division of a large relational table into smaller tables. This is done to divide the table across multiple machines, whether to lighten storage, or to improve performance.

	Horizontal sharding refers to dividing the table by rows, generally based on some index (such as sections based on a Name column, and sorted in alphabetical order: people with a name A to C on one machine, people with names starting from D to F, etc). For each horizontal shard, the row corresponding to the column names is (of course) duplicated.

	Vertical sharding refers to dividing across columns. For example, if a "person" entity has dozens or hundreds of attributes, we might divide these according to themes that should be analyzed in common (say we have a table with 81 columns, and we divide it as 1 primary key attribute, 10 attributes for professional situation, 20 attributes for medical history, 30 attributes for personal interests, and 20 attributes for consumer behavior; we could then vertically shard into a total of 4 new tables, stored on 4 different machines). The primary key of the original table is duplicated across all vertical shards. A join can then be used to obtain the rows of the original table, with all their original attributes.

\end{itemize}



\subsubsection*{Computer Science: Python programming for Data Science and Machine Learning}

This section presents the various tools that programmers who work in data science and machine learning use (most of the time). We explicitly ignore the R programming language, since the goal is to prepare the student to Machine Learning.

\begin{itemize}

	\item \textbf{Python (A)}: Python is a programming language designed to look a lot like plain English, to be simple to learn, to be dynamically typed, to be interpreted, and to have a richness of syntactic expression. This makes it a very good language for discovery through code and experimentation/prototyping. However, this language is sometimes quite slow at runtime, not easy to develop cross-platform, and dynamically typed. This makes it often avoided in production-grade environment, unless special care is taken. Python is above all the language of research, learning, scripting, and prototyping.

	\item \textbf{Jupyter (AMELTO)}: Technology used to launch a local server which makes one capable of running Python in a browser. It is very useful for easy Python development, given the problems of cross-platform distribution of Python, as well as those in the production of Python executables.

	\item \textbf{Numpy (AMELT)}: Standard linear algebra library in Python.

	\item \textbf{Scipy (AMELT)}: Standard scientific computing library in Python.

	\item \textbf{SciKit-Learn (AMELT)}: Standard Machine Learning library in Python.

	\item \textbf{PyTorch (AMELT)}: A sort of combination of Numpy and SciKit-Learn which has in recent years become the new standard for Machine Learning development, and increasingly general data science as well.

	\item \textbf{Pandas (AMELT)}: Library for managing dataframes (data tables) in Python. Very useful, quite standard in data science.

	\item \textbf{Polars (AMELT)}: Library for managing dataframes (data tables) in Python. Very useful, less famous, but more performant than Pandas.

	\item \textbf{Matplotlib (AMELT)}: Complete, but not very pretty, data visualization library in Python. A bit complex to get into at first, but essential to know.

	\item \textbf{Seaborn (AMELT)}: Data visualization library specialized in data science. Does every visualization task that is very "classical" quite well, simply, and beautifully. It is however sometimes difficult or impossible to do data visualizations with it, if these visualizations are too custom.

\end{itemize}




\subsection*{Math for Data Science}

\subsubsection*{Math for Data Science: Abstract Algebra}

This section describes the fundamentals that one tends to see during the first weeks of an undergrad of mathematics. While mastery is not essential for machine learning, understanding the following concepts makes the learning of all the mathematics related to data science much, much easier.

\begin{itemize}

	\item \textbf{Set (A)}: a set is a collection of mathematical objects. For example, the binary alphabet is the set containing the symbols $0$ and $1$, noted $\{0, 1\}$. The natural numbers form another set, noted $\mathbb{N} = \{0, 1, 2, 3, \dots\}$. You can think of a set as a deck of cards are unique, and there might be an infinite amount of cards.

	\item \textbf{Space / algebraic structure (A)}: a space is the combination of a set and with some rules/laws/properties and/or operators on its elements. For example, the '+' operator on the binary language (set of all combinations of symbols of the binary alphabet noted $\{0, 1\}^* = \{"", "0", "1", "00", "01", \\ "10", "11", \ldots\}$) is concatenation, and glues the "words" together (e.g., \texttt{"1010" + "111" = "1010111"}). The '+' operator in natural numbers is addition (e.g., $5 + 7 = 7 + 5 = 12$). An example of a property is commutativity: $a + b = b + a$. This property is verified in the natural integers but not in the binary language. You can think of properties as the rules you want to choose to play with your chosen deck of cards.

	\item \textbf{Function (mathematics) (A)}: a function is an association of the elements of a set of inputs (called a domain) with a set of outputs (called a codomain), where each input has at most one output image, so that the result of a function, given a fixed input, is deterministic. The square root is an example of a "1 input, 1 output" function. The classical operators (like addition) are examples of "2 input, 1 output" functions (for example, written in this way $+(5, 7) = 5 + 7 = 12$, this fact becomes clearer).

	\item \textbf{Application / Map (AG)}: You can consider that "application"/"map" and "function" are synonymous terms in mathematics (by abuse of language, even if there is a technical distinction between the two in French).

	\item \textbf{Image (AMELT)}: the image $y$ by a function $f$ of an element $x$ of the domain is the unique element $y = f(x)$ of the codomain. Put more simply, the "image" is the result value of applying the function, for a given input/output pair.

	\item \textbf{Antecedent (AMELT)}: an antecedent $x$ of an element $y$ of the codomain of a function $f$ is an element of the domain such that $y = f(x)$. Put more simply, the "antecedent" is the "origin" of some result of the function, for a given input/output pair.

	\item \textbf{Variable (mathematics) (A)}: a variable is a value which is named and typed (declared to be a member of a certain space) but not specified. This makes it so that this named value can act as a placeholder, to represent "any value" from the space in which it exists.

	\item \textbf{Parameter (A)}: a parameter is a variable which is given a specific value, for experimentation's sake, but which could just as easily be changed.

	\item \textbf{Function composition (A)}: if $f$ is a function from $A$ to $B$ and $g$ is a function from $B$ to $C$, then there exists a function $h = g \circ f$ from $A$ to $C$, such that $h$ is the concatenation of $f$ and then $g$ (ie, first applying $f$, then $g$), where the return value of the function $f$ is given as an argument to $g$.

	\item \textbf{Continuous function (A)}: a function is said to be continuous if its graphical representation (linking its inputs and outputs) has no "break".

	\item \textbf{Derivable function (A)}: a derivable function is a continuous function which does not have any "sharp corners" in its graphical representation.

	\item \textbf{Derivative (A)}: the derivative of a function is another function, representing the slope of rise or fall at each point of this function as a numerical value. A rise for an input point corresponds to a positive value of the derivative at that same input point; a fall corresponds to a negative value at that point.

	\item \textbf{Exponential function (A)}: the exponential function is the "fundamental" function whose derivative is the exponential function itself. Among its many properties, we have $e^{a + b} = e^a \cdot e^b$, i.e., it transforms an addition of inputs into a multiplication of outputs.

	\item \textbf{Logarithmic function / Neperian logarithm (A)}: the reciprocal of the exponential function. The derivative of the neperian logarithm is the function $(x \to \frac{1}{x})$. Also, $\ln(a \cdot b) = \ln(a) + \ln(b)$, i.e., the neperian logarithm turns a product into a sum.

	\item \textbf{Logistic function (AML, BET)}: The logistic function is a continuous, strictly increasing function, varying from 0 to 1 (in its outputs) from $-\infty$ to $+\infty$ (in its inputs).

	\item \textbf{Integral (A)}: the integral is the "ruler" (in the sense of a tool to measure) of mathematics. It is also the "anti-derivative".

	\item \textbf{Symbolic notation of iterated sums and products (AMELT)}: A large $\Sigma$ represents an iterated sum, a large $\Pi$ represents an iterated product. Example: $\sum_{i = 0}^{i = 5} 2^i = 2^0 + 2^1 + 2^2 + 2^3 + 2^4 + 2^5 = 1 + 2 + 4 + 8 + 16 + 32 = 63$.

\end{itemize}



\subsubsection*{Math for Data Science: Linear Algebra}

This is the meat of data science as a subject. All of machine learning, statistics, data science, most of physics, economics, etc, rely on linear algebra in some form or another.

\begin{itemize}

	\item \textbf{Vector space (AMELT)}: a vector space $E$ is an algebraic structure with commutative addition and subtraction (elements of the vector space, called \textbf{vectors}, behave like an "abelian group" under addition). It is always accompanied by another structure $K$ called "field", itself having usual addition, subtraction, multiplication and division (except by 0): this is the usual arithmetic that you've seen throughout school. Elements of $K$, called \textbf{scalars} allow one to change (scale) the size of vectors. This structure is coupled with a series of laws that allow the combined operation of $K$ and $E$ in a correct way.

	\item \textbf{Scalar (A)}: a scalar is an element of $K$. In general, we choose $K = \mathbb{R}$, the field of real numbers. A scalar is a tensor of rank 0.

	\item \textbf{Vector (A)}: a vector is an element of $E$. In general, a vector will be an array of $n$ elements. The number $n$ is then shared by all elements of $E$ and is called the "dimension" of $E$. $E$ is then denoted $\mathbb{R}^n$ (for the vector space of real numbers of dimension $n$). If $n$ can also be some infinite quantity. For example, the functions from $\mathbb{R}$ to $\mathbb{R}$ are also an example of vectors, because they respect the same laws (addition and scaling) within their enclosing space: the space $(\mathbb{R} \rightarrow \mathbb{R})$ is a vector space. A vector is a (contravariant) tensor of rank 1.

	\item \textbf{Vector addition (A)}: an operation representing addition of $E \times E \rightarrow E$ (2 inputs in $E$, one output in $E$). Addition is done by matching the basis elements of each operand, and summing each respective pair of coefficient scalar: e.g. $(1, 2, 3) + (10, 20, 30) = (11, 22, 33)$

	\item \textbf{Vector scaling (A)}: an operation which is a form multiplication of $K \times E \rightarrow E$ (1 input in $K$, 1 input in $E$, 1 output in $E$). It is done by multiplying all coordinates of a given vector by the same chosen scalar: e.g. $4 * (1, 2, 3) = (4, 8, 12)$.

	\item \textbf{Linear combination (AMLT, BE)}: any possible arrangement of scalings and additions of a set of vectors. For example, if we have 3 vectors $u$, $v$, and $w$, then $72 \cdot u + \frac{1}{3} \cdot v - 4.5 \cdot w$ is a linear combination of vectors $u$, $v$, and $w$. We can always simplify a linear combination back to a form where each vector is multiplied by a single scalar, and the list of these scalar-vector products is summed (as is used in the example).

	\item \textbf{Linear independence (AMELT)}: a set of $n$ vectors is linearly independent if the only way to get the zero vector (the zero of $E$, the element that changes nothing by vector addition) by linear combination is to have the scalar factor in front of each scalar-vector pair be zero. Intuitively, this means that each vector in the set contributes to creating a new dimension, independent of those created by the other vectors: there is no way to express one of the vectors as a linear combination of the others.

	\item \textbf{Basis (A)}: a set of linearly independent vectors that can describe any point in a vector space. There are always precisely as many elements in a basis as there are dimensions in a vector space.

	\item \textbf{Dimension (A)}: number of elements in any basis of a vector space; number of axes needed to represent a vector space.

	\item \textbf{Colinearity (AMELT)}: two vectors are said to be colinear if their direction describes the same line passing through the origin. This means that one can be scaled into the other.

	\item \textbf{Linearity / Linear application (AMELT)}: Said of a function from $E$ into $F$, where $E$ and $F$ are vector spaces, that sends the origin of $E$ (its zero vector) to the origin of $F$, and keeps all parallel lines in $E$ parallel in $F$. Algebraically, a function/application is said to be linear if and only if $f(0_E) = 0_F$ and $f(ku+v) = kf(u) + f(v)$, for all $k$ in $K$ and all $u$ and $v$ in $E$. Geometrically something "linear" is something straight (a line, a plane, etc); something non-linear is something curved (a sphere, a parabola, etc). "Linear" often also refers to polynomials of degree 1 (which have straight shapes), "quadratic" refers to polynomials of degree 2, "cubic" to polynomials of degree 3, etc.

	\item \textbf{Matrix (AMELTG)}: a matrix is a rectangle of scalars. A matrix of size $m \times n$ ($m$ rows, $n$ columns) represents a linear map of $\mathbb{R}^n \rightarrow \mathbb{R}^m$ (i.e., a function with as input a vector of dimension $n$, and as output a vector of dimension $m$). A matrix is a tensor of rank 2.

	\item \textbf{Matrix multiplication (AMELTG)}: matrix multiplication is the function composition of linear maps.

	\item \textbf{Dot product (A)}: the dot product is a product of $E \times E \rightarrow K$ (2 vectors of $E$ as input, one scalar of $K$ as output). It is computed as the sum of the term-to-term product of each coordinate of its two inputs. It is denoted $\langle u, v \rangle$, and is also equal to $\|u\| \cdot \|v\| \cdot \cos(u,v)$. Algebraically, for two vectors $u$ and $v$ of dimension $n$ with coordinates $u_i$ and $v_i$, the dot product is defined as: $\langle u, v \rangle = \sum_{i=1}^{i=n} u_i \cdot v_i$. The dot product algebraically encodes two pieces of geometric information (one about angles, and one about lengths) into a single number. In practice, the dot product of two vectors of norm 1 is equal to the angle between these two vectors; the dot product of two collinear vectors is equal to the multiplication of their norm.

	\item \textbf{Norm (A)}: The norm of a vector is the distance of displacement that this vector represents. It is usually defined using the dot product, as $\|u\| = \sqrt{\langle u, u \rangle}$ (Euclidean norm, also called the 2-norm, the one from the Pythagorean theorem). There are different norms (useful in ML), like the "supremum norm/$\infty$-norm" or the "Manhattan norm/1-norm".

	\item \textbf{Quadratic norm (AG)}: the quadratic norm is the norm of a vector squared. It is usually defined as $\|u\|^2 = \langle u, u \rangle$.

	\item \textbf{Cosine (A)}: value used to define the angle between 2 vectors. Algebraically, $\cos(u, v) = \frac{\langle u, v \rangle}{\|u\| \cdot \|v\|}$.

	\item \textbf{Distance (metric) (A)}: function to define a measure of the distance between two vectors. If the vector space is normed (has a norm function), a distance can always be defined from it as $d(u, v) = \|u - v\|$.

	\item \textbf{Span (generated vector subspace) (AMLT, BE)}: the vector subspace generated by a family (set) of vectors is the set of points that are reachable by linear combinations of the vectors of this family.

	\item \textbf{Normalization (linear algebra) (AMELT)}: vector scaling of a vector $u$ by the value $\frac{1}{\|u\|}$, in order to find a vector $\hat{u}$ of norm 1 and the same direction as $u$.

	\item \textbf{Tensor (AMELT)}: A tensor is an arrangement of scalars. A tensor of rank zero is a point of numbers (a single scalar). A tensor of rank one is a line of numbers: either a vector (column vector) or a covector (row vector). A tensor of rank two is a rectanle of numbers (a matrix). A tensor of rank three is a cube of numbers (hypermatrix). Etc.

	\item \textbf{Linear form / covector (AMELT)}: A linear form is a linear map of $E \rightarrow K$. Linear forms are row vectors.

	\item \textbf{Vector subspace (AMLT, BE)}: A vector subspace is a vector space contained in another vector space. A vector line, or a vector plane (i.e. passing through the origin) in a vector space of dimension 3 are examples of a vector subspace. Any space is a vector subspace of itself.

	\item \textbf{Hyperplane (AMLT, BE)}: A hyperplane is a vector subspace of dimension $n-1$ in a vector space of dimension $n$. The 2D planes are the hyperplanes of 3D space. The 1D lines are the hyperplanes of 2D space. The role of a hyperplane is to separate a vector space into exactly 2 pieces. For example, any "mirror" symmetry is done with respect to a hyperplane, whatever the dimension $n$ of the enclosing space.

	\item \textbf{Eigenvectors / Eigenvalues (BMLT, CE)}: Any linear map has vector subspaces that are stable (i.e. if an input is in that special stable subspace, then so is its output by the linear map in question). The are called eigenspaces. Linear maps in 1D eigenspaces amount to dilations (since both input and output are colinear). The vectors serving as the basis for these vector subspaces are called eigenvectors; their respective dilation coefficient is called an eigenvalue.

	\item \textbf{Euclidean space (AMLT, BE)}: vector space along with a dot product operator, allowing it to define a norm (the Euclidean norm) and therefore a metric (the Euclidean metric). This is the usual kind of space in which we do math and physics in high school.

	\item \textbf{Normed vector space (BMLT, CE)}: vector space along with a norm operator over vectors, allowing it to define a metric.

	\item \textbf{Metric space (BML, CET)}: vector space having a notion of distance between vectors (though not necessarily a notion of size for vectors).

\end{itemize}



\subsubsection*{Math for Data Science: Probability Theory}

This section describes the math of probability theory, which forms a lot of the backbone of statistics. Statistics is basically the combination of probability and linear algebra.

\begin{itemize}

	\item \textbf{Universe (of discourse) (A)}: set describing the collection of possible outcomes of a random experiment (some situation with a random outcome). For example, the universe of discourse for a single roll of a regular (six-sided) die (also called a "1d6") is the set $\Omega = \{ 1, 2, 3, 4, 5, 6 \}$.

	\item \textbf{Probability (A)}: A probability is a function over a universe of discourse that returns an outcome between 0 (ie, 0\% chance) and 1 (ie, 100\% chance). This universe of discourse is geometrically an object of measure 1 (a length of 1 in dimension 1, an area of 1 in dimension 2, a volume of 1 in dimension 3, an $n$-hypervolume of 1 in dimension $n$). The probability function assigns a measure between 0 and 1 to all collections of outcomes. For example, in the universe of discourse described above (the 1d6), the event "an even number is rolled" corresponds to the set $e = \{ 2, 4, 6 \}$ and is precisely one half of the full space (measure $0.5$), which corresponds to a 50\% chance.

	\item \textbf{Conditional probability (Bayesian probabilities) (A)}: A conditional probability is a probability considering/knowing that an event $e$ is necessarily true. This is geometrically equivalent to restricting oneself to the section of the universe of discourse where $e$ is true, and considering that this new geometric object is now of measure 1 (ie, certain). For example, take a 1d6, $e_1 = \{ 1, 2, 3, 4 \}$, ie, "rolling 4 or less", and $e_2 = \{ 2, 4, 6 \}$, ie, "rolling an even number". Then, the probability of "rolling an even number \textit{knowing that} we rolled 4 or less" is $0.5$, since the set $e_3 = \{ 2, 4 \}$ is half the measure of the the set $e_1 = \{ 1, 2, 3, 4 \}$ which is our new, "knowing that", universe. This should not be confused with the probability of "rolling an even number \textit{and} rolling 4 or less", which would be 1/3.

	\item \textbf{Random variable (AML, BETG)}: probabilistic experiment where the result is assigned a "success" value (in general a real number). Algebraically, a random variable is a function of the universe of discourse in (in general) the set of real numbers, usually denoted $X : \Omega \rightarrow \mathbb{R}$. For example, let's say I win 0.50 per point on a 1d6 die, but if I roll a 6, I instead lose 3. Our function looks like $X = \{ 1 \rightarrow 0.5 ; 2 \rightarrow 1 ; 3 \rightarrow 1.5 ; 4 \rightarrow 2 ; 5 \rightarrow 2.5 ; 6 \rightarrow -3 \}$.

	\item \textbf{Random vector (BML, CETG)}: probabilistic experiment where the outcome is assigned multiple values (usually $n$ real numbers). For example, if I refer to the value of two normal throws of the dice in order (2d6) as "x" for the first throw, and "y" for the second, then the example of a random vector where I win 2x candies but lose 0.5xy on each pair of throws can be defined as $X = (x, y) \rightarrow (2x, -0.5xy)$. Models in data science are usually random vectors that try to match the shape and properties of a cloud of data points, if the probabilistic experiment is performed repeatedly on different points.

	\item \textbf{Expectation (A)}: average of the results of a random variable over a universe, weighted by their probability. It corresponds to the "average return that can be expected in the long run, if we repeat the experiment an infinite amount of times".

	\item \textbf{Variance (A)}: Average of the squared deviations of each of the outcomes from the expectation. This gives an idea of the "spread" of the values of the experiment away from the average. The variance can also be understood as the covariance of a random variable (or vector) with itself, i.e. the quadratic norm of a random variable. In the case of a random vector, the variance takes the form of a symmetric, positive semidefinite matrix, called the variance-covariance matrix.

	\item \textbf{Standard deviation (A)}: the standard deviation is the square root of the variance of a variable or a random vector. It is therefore the norm of a random vector, noted $\sigma_X$.

	\item \textbf{Covariance (AML, BET, CG)}: The covariance is a measure of how much two random variables vary together. It is the dot product of spaces of random vectors. It is noted $\text{cov}(X, Y)$.

	\item \textbf{Pearson's correlation coefficient (AMLT, CG)}: a measure of how closely two events are correlated. It is the cosine of the spaces of random vectors. It is defined as $\text{cor}(X, Y) = \frac{\text{cov}(X, Y)}{\text{stddev}(X) \cdot \text{stddev}(Y)}$.

	\item \textbf{Bias (A)}: difference between the mean/expectation given by the model, and the one given by the experiment (the real data).

\end{itemize}



\subsubsection*{Math for Data Science: Multivariable calculus and differential geometry}

This is a more advanced mathematical subject, but it is necessary for more advanced topics in data science, such as the explanation for several advanced algorithms, explainability in ML, fields of data science like topological data analysis, etc.

\begin{itemize}

	\item \textbf{Scalar function (BMLT, CE)}: A scalar function is a function from $\mathbb{R}^n \rightarrow \mathbb{R}$ that is not necessarily linear (unlike linear forms). Ex: $f : \mathbb{R}^3 \rightarrow \mathbb{R}$, such that $(x, y, z) \rightarrow xy + z^2 + \cos(x)$.

	\item \textbf{Partial derivative (BMLT, CE)}: derivative of a single coordinate of scalar function. Each coordinate is obtained with the standard derivative, considering all coordinates constant, except one. E.g., going with the previous function, here is the partial derivative in the $x$ coordinate: $\pdv{f}{x} = y - \sin(x)$.

	\item \textbf{Differential / gradient (BMLT, CE)}: The differential / gradient is the "fundamental" derivative of a scalar function. The differential gradient indicates the direction in which to move to get the largest increase in the output value. The differential / gradient is calculated as the vector of all partial derivatives. The only difference between the two is that the differential (noted $df$) is a row vector (covector) and the gradient (noted $\nabla f$) is a column vector (i.e., their input/output space are reversed). E.g., going with the previous function:

	\begin{align*}
		df
		=
		\begin{bmatrix}
			\pdv{f}{x} & \pdv{f}{y} & \pdv{f}{z}
		\end{bmatrix}
		=
		\begin{bmatrix}
			y - \sin(x) & x & 2z
		\end{bmatrix}
	\end{align*}

	\item \textbf{Multivariate function (BMELT)}: a multivariate function is a function from $\mathbb{R}^n \rightarrow \mathbb{R}^m$, which is not necessarily linear. E.g.: $f : \mathbb{R}^3 \rightarrow \mathbb{R}^2$ such that $(x,y,z) \rightarrow (xy + \cos(x), xyz + z^2)$. Note that we can write $f(x,y,z) = (f_1(x,y,z), f_2(x,y,z))$, where $f_1(x,y,z) = xy + \cos(x)$ and $f_2(x,y,z) = xyz + z^2$ are both scalar functions.

	\item \textbf{Jacobian (BM, CELT)}: the "fundamental" derivative of a multivariate function. The Jacobian is a matrix of size $m \times n$, where the $i$-th row corresponds to the differential of the $i$-scalar function. To caricature, "we stack the differentials". Note that the Jacobian is itself a function from $\mathbb{R}^n \rightarrow (\mathbb{R}^n \rightarrow \mathbb{R}^m)$; that is, your Jacobian gives you a different matrix for each point in your space where you compute it. Geometrically, the Jacobian gives you the best linear approximation at a point $(x,y,z)$ of its return in the output space.
	\begin{align*}
		J_f
		=
		\begin{bmatrix}
			\pdv{f_1}{x} & \pdv{f_1}{y} & \pdv{f_1}{z} \\
			\pdv{f_2}{x} & \pdv{f_2}{y} & \pdv{f_2}{z}
		\end{bmatrix}
		=
		\begin{bmatrix}
			y - \sin(y) & x & 0 \\
			yz & xz & xy + 2z
		\end{bmatrix}
	\end{align*}

	\item \textbf{Manifold (AMLT, BEG)}: a manifold is an object which is "locally euclidean". This means that, if you zoom in enough on your object, it looks flat. It's a kind of object on which we can define a geometry through which differentiable functions can exist. Any vector space, as well as any space of functions between vector spaces, is a manifold. The sphere, the torus, the Klein bottle, the $n$-dimensional Cartesian plane ($\mathbb{R}^n$), are all examples of manifolds.

	\item \textbf{Topology (BMLT, CEG)}: a branch of mathematics that studies the local and global structures of manifolds. Also, a term for a "fabric of reality / choice of mathematical universe" based on the language of sets, on such spaces, in order to define notions of distance, measure, continuity, derivability, etc.

	\item \textbf{Measure theory (DML)}: extension of topology to build a more powerful version of the integral, and to define a way to compute lengths, areas, volumes, whatever the dimension. Probabilities are geometrically understood as "n-volumes relative to a global space of n-volume 1", and measure theory is thus the foundation of all modern probability theory.

	\item \textbf{Topological data analysis (DML)}: a technique that aims to study the geometric form (the manifold) on which data points exist in an encompassing statistical vector space, in order to drastically reduce its dimension or to make its logical structure explicit.

	\item \textbf{Differential Geometry (DML)}: theory of derivation on manifolds.

	\item \textbf{Information Geometry (DML)}: mathematical theory relying on probability, statistics, information theory, measure theory and differential geometry to establish statistical spaces (random vector spaces; data point cloud spaces) and dynamics on general manifolds.

\end{itemize}




\subsection*{Data science \& Machine Learning}

\subsubsection*{General vocabulary of statistics, data science and ML}

Here we define terms that are useful all around for scientific/technical data science and ML.

\begin{itemize}

	\item \textbf{Regression (AML, BETG)}: a type of model learning that seeks to predict numerical values for fictitious input data, close to the real output data in relation to its input data.

	\item \textbf{Classification (A)}: a type of model learning that seeks to predict the categorization of some output based on an input data point.

	\item \textbf{Model (A)}: a model is a mathematical function used as a hypothesis to generate a virtual data point from a chosen input. The goal of a model is to predict the actual data as closely as possible.

	\item \textbf{"Garbage in, garbage out" (A)}: a saying emphasizing the importance of quality data for accurate results in data science and machine learning.

	\item \textbf{R-squared (BML, CT)}: one of the measures of the validity of a model.

	\item \textbf{ML fields of study (AG)}: Natural Language Processing (NLP); Computer Vision; Decision-making; Business analytics; Ranking; Darwinian algorithms/Reinforcement learning, etc.

	\item \textbf{Neural network (A)}: a neural network is an architecture using linear algebra to build a graph of computational cells (neurons) and run an optimization protocol (part darwinian, part search for an optimum of a hard mathematical function) to allow an algorithm to become efficient for a given task in an unsupervised way.

	\item \textbf{Curse of dimensionality (AG)}: the growth of data points required for accurate representation grows exponentially with the number of attributes, making many algorithms inefficient for high-dimensional spaces.

	\item \textbf{Dimensionality reduction (AG)}: techniques used to reduce the dimension of high-dimensional spaces while preserving information. Used for visualization and to make data tractable for algorithms.

\end{itemize}



\subsubsection*{Machine Learning: Learning}

Here, we define the various terms that make up the "learning" part of machine learning.

\begin{itemize}

	\item \textbf{Supervised Learning (A)}: type of machine learning, where the data is all labeled and specified by a human being, so that the algorithm can use this as a foundation to know if it is right or wrong.

	\item \textbf{Unsupervised Learning / Self-Supervised Learning (A)}: learning where the algorithm itself is supposed to classify, measure or label the data on its own.

	\item \textbf{Semi-Supervised Learning (A)}: A learning approach where a model is trained on a mix of labeled and unlabeled data. It aims to leverage the unlabeled data to improve model performance.

	\item \textbf{Reinforcement Learning (A)}: learning where the algorithm itself perceives its environment, and learns according to it in a pseudo-darwinian way to perform a task more and more efficiently, based on some fitness/cost function.

	\item \textbf{Transfer Learning (BLG)}: A technique where a pre-trained model on one task is fine-tuned on a new, related task. This approach can significantly speed up the training process and improve performance, especially when labeled data is limited. Large Language Models, like GPT-3, are pre-trained on massive amounts of text data and then fine-tuned for specific tasks. This transfer learning approach allows them to perform well on a wide range of language-related tasks.

	\item \textbf{Fitness function / Cost function (A)}: a fitness/cost function is a function that, for a whole series of inputs, calculates the distance between the output of this input passed through the model function and the actual result in the data point cloud. The sum of these distances is the result of our cost function and is a measure of the accuracy of our model (compared to real world data). Machine learning algorithms try to improve fitness / minimize cost, to become better at their given problem. The cost function defines the "moral objective" of the AI. Choosing the right cost function is not just a mathematical or engineering question, but one of AI ethics as well.

	\item \textbf{Hyperparameters (AL, BMET)}: the parameters that allow one to manage the learning rate or direction of an algorithm.

	\item \textbf{Automated machine learning, AutoML (CLG)}: Automated Machine Learning refers to the process of automating the end-to-end process of applying machine learning to real-world problems, including data preprocessing, feature selection, model selection, and hyperparameter tuning.

	\item \textbf{Human-in-the-Loop (ALOG)}: Incorporating human reviewers to guide and oversee generated content (often for GANs or LLMs in particular) which helps ensure higher quality and ethical standards.

\end{itemize}



\subsubsection*{Data Science \& Machine Learning techniques}

Here we describe various techniques or meta-approaches that are common in data science and machine learning.

\begin{itemize}

	\item \textbf{Normalization (data science) (AML, BET)}: a change of benchmark used to rescale the distances between the points of a data point cloud, so as to express the same relative information between these data points, but so that a specific algorithm has better results.

	\item \textbf{Gradient descent (AML, BET)}: an algorithm using iterative gradient calculation to find an extremum of a complex mathematical function (typically, the minimization of a cost function).

	\item \textbf{Principal Components Analysis, PCA (BL, CMETOG)}: This is a dimensionality reduction technique (probably the best known and most widely used). Its principle is to express the underlying frame of reference of the data space in its most "expressive" form, i.e. that maximizes the variance of the data projected on the axes of the new frame of reference. This allows having a maximum of information on the data cloud with a minimum of dimensions (axes). The origin of this new frame is the mean of the data points, and the axes are computed by Singular Value Decomposition (SVD) of the covariance matrix. Projecting the data points onto the main axes allows visualizing an approximate but rather accurate version of a much more complex high-dimensional data space.

	\item \textbf{Decision trees, random forests (AML, BETO)}: machine learning techniques that build decision trees (conditional "if" trees) from data, used for classification and regression. They are easy for humans to interpret.

	\item \textbf{Kernel methods / Integral kernel / Window functions / Statistical Kernel / Reproducing kernel Hilbert space, RKHS (DMLT)}: methods using covectors of vector spaces of functions, or modified dot products, to transform the shape of data point clouds into something easier to handle (e.g., by linear methods) and achieve better results.

	\item \textbf{Gradient boosting (AL, BMET)}: An ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong predictive model. It builds the model in a stage-wise manner, focusing on correcting the errors of previous iterations. XGBoost is a popular open-source implementation of gradient boosting that is known for its efficiency and performance in machine learning competitions. LightGBM is another gradient boosting framework that is designed to be memory-efficient and fast, making it suitable for large datasets. CatBoost is a gradient boosting algorithm that handles categorical features automatically, eliminating the need for extensive preprocessing.

	\item \textbf{Attention mechanisms (CL)}: Techniques that allow models to focus on specific parts of input data when making predictions. These significantly improve the performance of NLP models.

	\item \textbf{Neural style transfer (CL)}: A technique that combines the content of one image with the artistic style of another image using neural networks.

	\item \textbf{Anomaly detection (BL, CMET)}: A field of machine learning focused on identifying rare events or outliers in data. It has applications in fraud detection, network security, and more.

\end{itemize}



\subsubsection*{Data Science \& Machine Learning algorithms}

\begin{itemize}

	\item \textbf{Linear regression (AG)}: a technique that allows establishing a model (here, an approximation of a cloud of data points) in the form of a vector subspace minimizing the distance to a set of points.

	\item \textbf{Logistic regression (AG)}: form of regression using the logistic function to obtain a probabilistic model of binary classification (true/false).

	\item \textbf{K-means clustering (AMELT)}: A classification algorithm that seeks to partition a group of n observations (an n-point data point cloud) into k "clusters" where each point belongs to the cluster with the closest mean. The algorithm organizes the points of the initial cloud by iteratively readjusting hyperplanes to converge to a local optimum.

	\item \textbf{K-nearest neighbors (AMELT)}: algorithm for both classification and regression, assigning to each point either the category of its k nearest neighbors (classification) or the average of the values assigned to these k nearest neighbors (regression). It is based on a choice of distance and requires normalization of distances.

	\item \textbf{Support Vector Machines, SVM (AL, BMET)}: supervised learning algorithm for linear classification using hyperplanes. Extensions exist for regression and non-linear classification (using kernel methods).

	\item \textbf{Naive bayes classifiers (AL, BME, CT)}: family of algorithms based on conditional probabilities to perform classification.

	\item \textbf{Multi-Layer Perceptron, multilayer perceptron, MLP (AG)}: a fundamental neural network architecture that takes input, processes through layers, and produces an output. Learning is done using the backpropagation algorithm.

	\item \textbf{Convolutional Neural Network, CNN (AL, BMETG)}: specialized neural network for image processing, using convolution cells to extract local shapes.

	\item \textbf{Recurrent Neural Networks, RNN (AL, BG)}: neural network architecture with linked non-neighboring layers that can affect each other. It is designed to handle sequences of data, making them suitable for tasks like time series prediction, natural language processing, and more.

	\item \textbf{Long Short Term Memory Network, LSTM (AL, BG)}: a type of complex Recurrent Neural Network, capable of learning and remembering longer sequences of data, used for NLP and speech recognition.

	\item \textbf{Large Language Model, LLM (A)}: Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text based on vast amounts of training data. LLMs, such as OpenAI's GPT-3, utilize deep learning techniques to learn the statistical patterns and structures of language. These models have the capacity to generate coherent and contextually relevant text, making them invaluable tools for natural language processing tasks. LLMs can perform a wide range of language-related tasks, including text generation, language translation, sentiment analysis, chatbot interactions, content summarization, and more. Their capabilities stem from the complex neural architectures they employ, which consist of multiple layers of interconnected processing units. LLMs have garnered significant attention for their potential to revolutionize various industries and applications by enhancing human-computer interactions and enabling sophisticated language-related tasks. [This definition was generated using ChatGPT with GPT 3.5]

	\item \textbf{Gated Recurrent Unit (GRU) (BL, CG)}: A variation of the LSTM architecture with fewer parameters, often used for similar tasks as LSTMs.

	\item \textbf{Generative Adversarial Networks, GAN (A)}: neural network architecture with two agents, a generator (which acts like forger) and a discriminator (which acts like an inspector), that improve by competing with each other. Used to produce realistic data in image, video or audio format.

	\item \textbf{Transformer Architecture (BL, CG)}: A neural network architecture designed for sequence-to-sequence tasks, such as machine translation. Transformers have revolutionized NLP and are the foundation of models like BERT, GPT, and T5.

	\item \textbf{Q-learning (AL, BMETG)}: Model-free reinforcement learning algorithm widely used for AIs, generally by having them "play video games" (react and evolve in a given simulated environment).

\end{itemize}




\subsection*{Other: Design}

\begin{itemize}

	\item \textbf{Reverse engineering (AG)}: The process of studying the functioning of a product or algorithm to understand its design and production. This can involve reproducing or improving upon the original design.

	\item \textbf{Divergent thinking (AG)}: The ability to think creatively and generate a wide range of possible solutions to a design problem. It involves thinking outside the box and considering unconventional ideas.

	\item \textbf{Convergent thinking (AG)}: The ability to systematically filter and evaluate possible solutions to a problem based on a defined set of constraints. It is similar to the scientific method (aka Cartesian method).

	\item \textbf{Design thinking (AG)}: A mental protocol for creative problem-solving that combines both approaches of divergent and convergent thinking.

	\item \textbf{Algorithm design (AG)}: The process of inventing protocols for automated data processing, involving the creation of step-by-step instructions for solving specific problems.

	\item \textbf{Database design (AG)}: The process of inventing a data model for a domain problem, and selecting the appropriate database architecture to model it.

	\item \textbf{System design (AG)}: The process of constructing an architecture using computers to solve a given problem.

\end{itemize}


\newpage



\section*{TODOs}

Add table of contents and index. Possibly add newpage between sections.

Improve difficulty / importance code usage (right now, it uses A and B too much; many Bs should be Cs for specific profiles, or a G should be added). Probably divide Importance and Difficulty into 2 different codes. Probably use emojis for the code rather than letters, so that they can be search for easily with Ctrl+F by people of the respective profile.

Improve NoSQL section.

Maybe improve mathematical section to always have, for each term:
- the ELI5 definition
- an example
- a non-example
- the intuitive technical definition (may be the same as the ELI5 one)
- the rigorous, formal definition
? Or just keep an ELI5+ and send people to go read Mathophilia instead ?

Add forgotten words:
- XML
- JSON
- HTML
- backpropagation
- overfitting
- TLS
- SSL
- map
- fold / reduce / join
- monad
- algorithmic complexity (space, time)
- hashing
- SHA
- precision / recall
- false positive / false negative
- datacenter
- cloud
- content distribution network
- devops
- devsecops
- mlops
- hex arch
- opsec (make security section / S code ?) 
- osint
- Apache (Kafka, Hadoop, Spark, Hive, Cassandra)
- MapReduce



Add section on classical algorithms and algorithmic constructs, not just basic data structures (possibly drawing analogies between discrete and continuous versions where applicable). Improve the subdivision of sections on algorithmics.
- BFS
- DFS
- Floyd-Warshall
- graph matrices (adjacency, self-similarity, laplacian)
- Fourier transform, FFT
- Markov chains
- Monte-carlo integration

Increase the quality of explanation of algorithms and data structures with images and more details, all throughout the lexicon.

Add section on system engineering algorithms
-> distributed hashing
-> consistent hashing
-> geohash
-> quadtree / octree etc
-> Leaky bucket
-> Trie
-> Bloomfilter
-> Raft/Paxos

Add section on data visualizations
-> Histograms
-> Bar Charts
-> Line Charts
-> Scatter Plots
-> Heatmaps
-> Box Plots
-> Choropleth Maps
-> Tree Maps
-> Sankey Diagrams
-> Word Clouds
-> Parallel Coordinates

Add section specific to dimensionality reduction
- Advanced dimensionality reduction algorithms:
-> Principal component analysis, PCA
-> t-distributed stochastic neighbor embedding, t-SNE 
-> Uniform Manifold Approximation and Projection, UMAP
-> Locally Linear Embedding, LLE
-> Multi-dimensional scaling, MDS
-> Isometric mapping, Isomap
-> Non-negative Matrix Factorization, NMF
-> RadViz
- Neural nets for dimensionality reduction
-> Autoencoders
-> Self-Organizing Maps, SOMs

Add "Other: legal" section
-> free software / freeware
-> open source
-> closed source
-> licenses (MIT, Apache, etc)
-> definition of laws like RGPD and what they entail

\end{document}
